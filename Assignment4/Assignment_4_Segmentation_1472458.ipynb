{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Assignment_4_Segmentation_1472458.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"code","metadata":{"id":"EHD1DOQVACzP","executionInfo":{"status":"ok","timestamp":1604041852309,"user_tz":360,"elapsed":20711,"user":{"displayName":"菅泳钦","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisNkEPQ1MIyihCSP3C7W28PHOGAXAyhKjVek8T=s64","userId":"04446447163478496729"}},"outputId":"ea445611-1ea3-48ca-d1eb-f22a680d64e8","colab":{"base_uri":"https://localhost:8080/"}},"source":["# Student Name: Yongqin Jian\n","# Student ID: 1472458\n","import os\n","import time\n","\n","import torch.nn.functional as F\n","import torch\n","from torch import nn\n","from torchvision import models\n","import numpy as np\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#Edit root_dir here to point where the TextureImagesDataset folder \n","root_dir = \"drive/My Drive/Cmput 328/Assignment4/Semantic Segmentation\"\n","\n","class TextureImages(object):\n","    def __init__(self, subset='train', batch_size=64, shuffle=True):\n","        if subset == 'train':\n","            images = np.load(os.path.join(root_dir, 'TextureImagesDataset',\n","                                          'train_images.npy'))\n","            masks = np.load(os.path.join(root_dir, 'TextureImagesDataset',\n","                                         'train_masks.npy'))\n","        elif subset == 'test':\n","            images = np.load(os.path.join(root_dir, 'TextureImagesDataset',\n","                                          'test_images.npy'))\n","            masks = np.load(os.path.join(root_dir, 'TextureImagesDataset',\n","                                         'test_masks.npy'))\n","        else:\n","            raise NotImplementedError\n","        self._images = images\n","        self.images = self._images\n","        self._masks = masks\n","        self.masks = self._masks\n","        self.batch_size = batch_size\n","        self.num_samples = len(self.images)\n","        self.shuffle = shuffle\n","        if self.shuffle:\n","            self.shuffle_samples()\n","        self.next_batch_pointer = 0\n","\n","    def shuffle_samples(self):\n","        image_indices = np.random.permutation(np.arange(self.num_samples))\n","        self.images = self._images[image_indices]\n","        self.masks = self._masks[image_indices]\n","\n","    def get_next_batch(self):\n","        num_samples_left = self.num_samples - self.next_batch_pointer\n","        if num_samples_left >= self.batch_size:\n","            x_batch = self.images[self.next_batch_pointer:self.next_batch_pointer + self.batch_size]\n","            y_batch = self.masks[self.next_batch_pointer:self.next_batch_pointer + self.batch_size]\n","            self.next_batch_pointer += self.batch_size\n","        else:\n","            x_partial_batch_1 = self.images[self.next_batch_pointer:self.num_samples]\n","            y_partial_batch_1 = self.masks[self.next_batch_pointer:self.num_samples]\n","            if self.shuffle:\n","                self.shuffle_samples()\n","            x_partial_batch_2 = self.images[0:self.batch_size - num_samples_left]\n","            y_partial_batch_2 = self.masks[0:self.batch_size - num_samples_left]\n","            x_batch = np.vstack((x_partial_batch_1, x_partial_batch_2))\n","            y_batch = np.vstack((y_partial_batch_1, y_partial_batch_2))\n","            self.next_batch_pointer = self.batch_size - num_samples_left\n","        return x_batch, y_batch\n","\n","class CrossEntropyLoss2d(nn.Module):\n","    def __init__(self, weight=None, size_average=True, ignore_index=255):\n","        super(CrossEntropyLoss2d, self).__init__()\n","        self.nll_loss = nn.NLLLoss(weight, size_average, ignore_index)\n","\n","    def forward(self, inputs, targets):\n","        return self.nll_loss(F.log_softmax(inputs, dim=1), targets)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RSyJsklzzojQ"},"source":["def SemSeg(input_size, num_classes=5):\n","    # TODO: Implement Semantic Segmentation network here\n","    # Returned logits must be a tensor of size:\n","    # (<batch_size>, image_height, image_width, num_classes + 1)\n","    # 1st dimension is batch dimension\n","    # image_height and image_width are the height and width of input_tensor\n","    # last dimension is the softmax dimension. There are 4 texture classes plus 1 background class\n","    # therefore last dimension will be 5\n","\n","    #****************************************** Documentation ***********************************************#\n","    # This net work is build with Convolution and Transpose Convolution, I use 3 encode layers\n","    # and 2 decode layers here. Literally, it down sampling the input pictures with more than 3\n","    # channels, which allows the nn model to learn how to deal with each pixel, like it uses \n","    # specific filters to munipulate each pixel. By down sampling/resolution reduce, the model is \n","    # albe to segment element of the image, specifically for each pixel, it represent a class of\n","    # elements. Then, by using Transpose convolution, it increase the resolution/up sampling, makes \n","    # the output size equal to input size. The transpose convolution takes each pixel of the \n","    # bottleneck into a filter, and paste into the output to increase the resolution.\n","    # Furthermore, in this nn model,the forward function, I used 3 encoder layers, and applied\n","    # down sampling during each convolution layer, and reaches 128x20x20 at the bottleneck, then\n","    # I used 2 transpose convolution layers to up sampling to reach 5x196x196 at the output.\n","    # However, the accuary only reaches around 94%. Meanwhile, I also found that after 1st run of\n","    # this notebook, my accuracy would decrease to 7x% for all following run. And I cound't figure\n","    # out why is that. So during marking, I assume the accuacy is based on the first time run of \n","    # this Notebook. \n","    # during training, I set batch number to 10 for better perfomence, others remain the same\n","    # the input_size is not being used in this model, because convolutional network does not take\n","    # the input size as a parameter.\n","\n","    class Net(nn.Module):\n","      def __init__(self):\n","        super(Net,self).__init__()\n","        self.conv1 = nn.Conv2d(3,32,kernel_size = 9)\n","        # max pool => 94\n","        self.conv2 = nn.Conv2d(32,64,kernel_size = 7)\n","        # max pool => 44\n","        self.conv3 = nn.Conv2d(64,128,kernel_size = 5)\n","        # max pool => 20\n","\n","        self.conv4 = nn.ConvTranspose2d(in_channels = 128,\n","                          out_channels=32,\n","                          kernel_size = 7,\n","                          stride = 3)\n","        \n","        self.conv5 = nn.ConvTranspose2d(in_channels = 32,\n","                          out_channels = 5,\n","                          kernel_size = 7,\n","                          stride = 3)\n","        \n","      def forward(self,x):\n","        x = F.relu(self.conv1(x))\n","        x = F.max_pool2d(x,kernel_size = 2,stride = 2)\n","\n","        x = F.relu(self.conv2(x))\n","        x = F.max_pool2d(x,kernel_size = 2,stride = 2)\n","\n","        x1 = x\n","        x = F.relu(self.conv3(x))\n","        x = F.max_pool2d(x,kernel_size = 2,stride = 2)\n","\n","        x = F.relu(self.conv4(x))\n","        x = F.relu(self.conv5(x)).squeeze()\n","        return F.log_softmax(x,dim = 1)\n","\n","    model = Net()\n","\n","   \n","    return model\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H0wpkK7d9zWJ","executionInfo":{"status":"ok","timestamp":1604042131155,"user_tz":360,"elapsed":299542,"user":{"displayName":"菅泳钦","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisNkEPQ1MIyihCSP3C7W28PHOGAXAyhKjVek8T=s64","userId":"04446447163478496729"}},"outputId":"fd4aa8af-fc02-4e60-a0d4-513f7bddfe8b","colab":{"base_uri":"https://localhost:8080/"}},"source":["def run():\n","    # You can tune the hyperparameters here.\n","    n_epochs = 25\n","    batch_size = 10\n","    learning_rate = 0.001\n","    weight_decay = 0.001\n","    use_cuda = 1\n","\n","    load_weights = 0\n","    wts_fname = 'model.pt'\n","\n","    input_size = (196, 196)\n","    n_batches = int(2000 / batch_size)\n","    wts_path = os.path.join(root_dir, wts_fname)\n","\n","    if use_cuda and torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","        print('Training on GPU: {}'.format(torch.cuda.get_device_name(0)))\n","    else:\n","        device = torch.device(\"cpu\")\n","        print('Training on CPU')\n","\n","    train_set = TextureImages('train', batch_size=batch_size)\n","    test_set = TextureImages('test', shuffle=False)\n","\n","    model = SemSeg(input_size).to(device)\n","\n","    def evaluation(images, true_labels):\n","        eval_batch_size = 100\n","        predicted_labels = []\n","        model.eval()\n","        with torch.no_grad():\n","            for start_index in range(0, len(images), eval_batch_size):\n","                end_index = start_index + eval_batch_size\n","                batch_x = images[start_index: end_index]\n","                # batch_x = np.reshape(batch_x, (batch_x.shape[0], 3, 196, 196))\n","                batch_x = torch.FloatTensor(batch_x).permute((0, 3, 1, 2)).to(device)\n","                batch_predicted_logits = model(batch_x)\n","                batch_predicted_labels = torch.argmax(batch_predicted_logits, axis=1)\n","                batch_predicted_labels = batch_predicted_labels.cpu().numpy()\n","                predicted_labels += list(batch_predicted_labels)\n","        predicted_labels = np.vstack(predicted_labels).flatten()\n","        true_labels = true_labels.flatten()\n","        accuracy = float((predicted_labels == true_labels).astype(np.int32).sum()) / true_labels.size\n","        return predicted_labels, accuracy\n","\n","    if load_weights:      \n","        print('Loading weights from: {}'.format(wts_path))\n","        chkpt = torch.load(wts_path, map_location=device)  # load checkpoint\n","        model.load_state_dict(chkpt['model'])\n","    else:\n","        criterion = CrossEntropyLoss2d().to(device)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n","\n","        print(\"Training...\")\n","        mean_loss = 0\n","        steps = 0\n","        losses = []\n","        max_accuracy = 0\n","        max_accuracy_id = 0\n","        for epch_id in range(n_epochs):\n","            model.train()\n","            for batch_id in range(n_batches):\n","                start_t = time.time()\n","\n","                batch_x, batch_y = train_set.get_next_batch()\n","\n","                batch_x = torch.FloatTensor(batch_x).permute((0, 3, 1, 2)).to(device)\n","                batch_y = torch.LongTensor(batch_y).squeeze().to(device)\n","\n","                optimizer.zero_grad()\n","\n","                outputs = model(batch_x)\n","                loss = criterion(outputs, batch_y)\n","                loss.backward()\n","                optimizer.step()\n","\n","                end_t = time.time()\n","\n","                _loss = loss.item()\n","                steps += 1\n","                mean_loss += (_loss - mean_loss) / steps\n","                losses.append(_loss)\n","\n","                time_taken = end_t - start_t\n","\n","                #print('batch: {} / {} loss: {} mean_loss: {} time_taken: {}'.format(\n","                #    batch_id, n_batches, _loss, mean_loss, time_taken))\n","\n","            _, test_accuracy = evaluation(test_set._images, test_set._masks)\n","            if test_accuracy > max_accuracy:\n","                max_accuracy = test_accuracy\n","                max_accuracy_id = epch_id\n","                chkpt = {\n","                    'model': model.state_dict(),\n","                }\n","                torch.save(chkpt, '{}.{}'.format(wts_path, max_accuracy_id))\n","            print(\"epch {} / {}: Test Pixel Accuracy = {:.3f} max_accuracy = {:.3f} in epoch {}\".format(\n","                epch_id + 1, n_epochs, test_accuracy, max_accuracy, max_accuracy_id + 1))\n","        print(\"Done training. Weights saved to: {}\".format(wts_fname))\n","        chkpt = {\n","            'model': model.state_dict(),\n","        }\n","        torch.save(chkpt, wts_path)\n","\n","    print('Evaluating on test set')\n","    _, test_accuracy = evaluation(test_set._images, test_set._masks)\n","    print(\"Test Pixel Accuracy = {:.3f}\".format(test_accuracy))\n","    return test_accuracy\n","\n","\n","if __name__ == '__main__':\n","    run()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training on GPU: Tesla T4\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='mean' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Training...\n","epch 1 / 25: Test Pixel Accuracy = 0.883 max_accuracy = 0.883 in epoch 1\n","epch 2 / 25: Test Pixel Accuracy = 0.932 max_accuracy = 0.932 in epoch 2\n","epch 3 / 25: Test Pixel Accuracy = 0.937 max_accuracy = 0.937 in epoch 3\n","epch 4 / 25: Test Pixel Accuracy = 0.937 max_accuracy = 0.937 in epoch 3\n","epch 5 / 25: Test Pixel Accuracy = 0.941 max_accuracy = 0.941 in epoch 5\n","epch 6 / 25: Test Pixel Accuracy = 0.942 max_accuracy = 0.942 in epoch 6\n","epch 7 / 25: Test Pixel Accuracy = 0.945 max_accuracy = 0.945 in epoch 7\n","epch 8 / 25: Test Pixel Accuracy = 0.944 max_accuracy = 0.945 in epoch 7\n","epch 9 / 25: Test Pixel Accuracy = 0.941 max_accuracy = 0.945 in epoch 7\n","epch 10 / 25: Test Pixel Accuracy = 0.943 max_accuracy = 0.945 in epoch 7\n","epch 11 / 25: Test Pixel Accuracy = 0.944 max_accuracy = 0.945 in epoch 7\n","epch 12 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 12\n","epch 13 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 13\n","epch 14 / 25: Test Pixel Accuracy = 0.945 max_accuracy = 0.946 in epoch 13\n","epch 15 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 13\n","epch 16 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 16\n","epch 17 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 16\n","epch 18 / 25: Test Pixel Accuracy = 0.945 max_accuracy = 0.946 in epoch 16\n","epch 19 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 19\n","epch 20 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 19\n","epch 21 / 25: Test Pixel Accuracy = 0.947 max_accuracy = 0.947 in epoch 21\n","epch 22 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.947 in epoch 21\n","epch 23 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.947 in epoch 21\n","epch 24 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.947 in epoch 21\n","epch 25 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.947 in epoch 21\n","Done training. Weights saved to: model.pt\n","Evaluating on test set\n","Test Pixel Accuracy = 0.946\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LCmvHbf27Tmr"},"source":["#**`My Own Training Output`**\n","Training on GPU: Tesla T4\n","\n","Training...\n","epch 1 / 25: Test Pixel Accuracy = 0.681 max_accuracy = 0.681 in epoch 1\n","\n","epch 2 / 25: Test Pixel Accuracy = 0.929 max_accuracy = 0.929 in epoch 2\n","\n","epch 3 / 25: Test Pixel Accuracy = 0.940 max_accuracy = 0.940 in epoch 3\n","\n","epch 4 / 25: Test Pixel Accuracy = 0.942 max_accuracy = 0.942 in epoch 4\n","\n","epch 5 / 25: Test Pixel Accuracy = 0.944 max_accuracy = 0.944 in epoch 5\n","\n","epch 6 / 25: Test Pixel Accuracy = 0.943 max_accuracy = 0.944 in epoch 5\n","\n","epch 7 / 25: Test Pixel Accuracy = 0.943 max_accuracy = 0.944 in epoch 5\n","\n","epch 8 / 25: Test Pixel Accuracy = 0.944 max_accuracy = 0.944 in epoch 8\n","\n","epch 9 / 25: Test Pixel Accuracy = 0.945 max_accuracy = 0.945 in epoch 9\n","\n","epch 10 / 25: Test Pixel Accuracy = 0.945 max_accuracy = 0.945 in epoch 9\n","\n","epch 11 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 11\n","\n","epch 12 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 11\n","\n","epch 13 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 11\n","\n","epch 14 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 11\n","\n","epch 15 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 11\n","\n","epch 16 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 11\n","\n","epch 17 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 11\n","\n","epch 18 / 25: Test Pixel Accuracy = 0.944 max_accuracy = 0.946 in epoch 11\n","\n","epch 19 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.946 in epoch 11\n","\n","epch 20 / 25: Test Pixel Accuracy = 0.947 max_accuracy = 0.947 in epoch 20\n","\n","epch 21 / 25: Test Pixel Accuracy = 0.945 max_accuracy = 0.947 in epoch 20\n","\n","epch 22 / 25: Test Pixel Accuracy = 0.947 max_accuracy = 0.947 in epoch 20\n","\n","epch 23 / 25: Test Pixel Accuracy = 0.947 max_accuracy = 0.947 in epoch 20\n","\n","epch 24 / 25: Test Pixel Accuracy = 0.916 max_accuracy = 0.947 in epoch 20\n","\n","epch 25 / 25: Test Pixel Accuracy = 0.946 max_accuracy = 0.947 in epoch 20\n","\n","Done training. Weights saved to: model.pt\n","\n","Evaluating on test set\n","\n","Test Pixel Accuracy = 0.946\n"]},{"cell_type":"markdown","metadata":{"id":"IBaXJdvg_ZXV"},"source":["#Possible Output \n","Training on GPU: Tesla K80\n","\n","Training...\n","\n","epch 1 / 25: Test Pixel Accuracy = 0.865 max_accuracy = 0.865 in epoch 1\n","\n","epch 2 / 25: Test Pixel Accuracy = 0.945 max_accuracy = 0.945 in epoch 2\n","\n","epch 3 / 25: Test Pixel Accuracy = 0.938 max_accuracy = 0.945 in epoch 2\n","\n","epch 4 / 25: Test Pixel Accuracy = 0.975 max_accuracy = 0.975 in epoch 4\n","\n","epch 5 / 25: Test Pixel Accuracy = 0.981 max_accuracy = 0.981 in epoch 5\n","\n","epch 6 / 25: Test Pixel Accuracy = 0.982 max_accuracy = 0.982 in epoch 6\n","\n","epch 7 / 25: Test Pixel Accuracy = 0.583 max_accuracy = 0.982 in epoch 6\n","\n","epch 8 / 25: Test Pixel Accuracy = 0.959 max_accuracy = 0.982 in epoch 6\n","\n","epch 9 / 25: Test Pixel Accuracy = 0.762 max_accuracy = 0.982 in epoch 6\n","\n","epch 10 / 25: Test Pixel Accuracy = 0.864 max_accuracy = 0.982 in epoch 6\n","\n","epch 11 / 25: Test Pixel Accuracy = 0.941 max_accuracy = 0.982 in epoch 6\n","\n","epch 12 / 25: Test Pixel Accuracy = 0.963 max_accuracy = 0.982 in epoch 6\n","\n","epch 13 / 25: Test Pixel Accuracy = 0.954 max_accuracy = 0.982 in epoch 6\n","\n","epch 14 / 25: Test Pixel Accuracy = 0.821 max_accuracy = 0.982 in epoch 6\n","\n","epch 15 / 25: Test Pixel Accuracy = 0.846 max_accuracy = 0.982 in epoch 6\n","\n","epch 16 / 25: Test Pixel Accuracy = 0.967 max_accuracy = 0.982 in epoch 6\n","\n","epch 17 / 25: Test Pixel Accuracy = 0.945 max_accuracy = 0.982 in epoch 6\n","\n","epch 18 / 25: Test Pixel Accuracy = 0.971 max_accuracy = 0.982 in epoch 6\n","\n","epch 19 / 25: Test Pixel Accuracy = 0.985 max_accuracy = 0.985 in epoch 19\n","\n","epch 20 / 25: Test Pixel Accuracy = 0.980 max_accuracy = 0.985 in epoch 19\n","\n","epch 21 / 25: Test Pixel Accuracy = 0.986 max_accuracy = 0.986 in epoch 21\n","\n","epch 22 / 25: Test Pixel Accuracy = 0.988 max_accuracy = 0.988 in epoch 22\n","\n","epch 23 / 25: Test Pixel Accuracy = 0.989 max_accuracy = 0.989 in epoch 23\n","\n","epch 24 / 25: Test Pixel Accuracy = 0.982 max_accuracy = 0.989 in epoch 23\n","\n","epch 25 / 25: Test Pixel Accuracy = 0.987 max_accuracy = 0.989 in epoch 23\n","\n","Done training. Weights saved to: model.pt\n","\n","Evaluating on test set\n","\n","Test Pixel Accuracy = 0.987"]}]}