{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Partial_Derivatives_and_GD_and_chain_rule_new.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CYz-EHRRjR5s"},"source":["# Multivariate function and its derivative"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iBdw1ckMUomp","colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"status":"ok","timestamp":1599696655758,"user_tz":360,"elapsed":3865,"user":{"displayName":"Nilanjan Ray","photoUrl":"","userId":"13266317426275772701"}},"outputId":"c9ee6412-fedf-4cb8-a21c-0a0de91a49a4"},"source":["import torch\n","from torch.autograd import Variable\n","from torch.autograd import Function\n","import torch.nn.functional as F\n","import numpy as np\n","\n","x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n","f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]+2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n","\n","print(\"Vector variable x:\",x.data)\n","\n","print(\"Function f at x:\",f.data)\n","\n","# compute gradient of f at x\n","g = torch.autograd.grad(f,x)\n","\n","print(\"Gradient of f at x:\",g[0].data)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Vector variable x: tensor([ 3., -1.,  0.,  1.])\n","Function f at x: tensor(215.)\n","Gradient of f at x: tensor([ 306., -144.,  -18., -310.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hdy43US97dGS","colab_type":"text"},"source":["### Let's verify with math formula.\n","\n","$f(x_1,x_2,x_3,x_4) = (x_1+10x_2)^2 + 5(x_3-x_4)^2 + (x_2+2x_3)^4 + 10(x_1-x_4)^4$\n","\n","Let's evaluate these partial derivatives at\n","\n","$x_1 = 3, x_2 = -1, x_3 = 0, x_4 = 1$\n","\n","$\\frac{\\partial f}{\\partial x_1} = 2(x_1+10x_2)+40(x_1-x_4)^3 = 2(3-10)+40(3-1)^3 = -14+320 = 306$\n","\n","$\\frac{\\partial f}{\\partial x_2} = 20(x_1+10x_2)+4(x_2+2x_3)^3 = 20(3-10)+4(-1)^3 = -140-4 = -144$\n","\n","$\\frac{\\partial f}{\\partial x_3} = 10(x_3-x_4)+8(x_2+2x_3)^3 = 10(-1)+8(-1)^3 = -10-8 = -18$\n","\n","$\\frac{\\partial f}{\\partial x_4} = -10(x_3-x_4)-40(x_1-x_4)^3 = -10(-1)-40(3-2)^3 = 10-320 = -310$\n","\n","So, we have\n","\n","$\\nabla f(3,-1,0,1) = \\left[ 306,-144,-18,-310 \\right]$\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iIcDkF7CDAVi"},"source":["# Numerical Derivative\n","### For any function we can also compute its numerical derivative, which is an approximation. The formula for computing numerical derivatives often uses Taylor series approximation from Calculus:\n","$\\frac{\\partial f}{\\partial x_1} = \\frac{f(x_1+\\delta x_1,x_2,x_3,...)-f(x_1-\\delta x_1,x_2,x_3,...)}{2\\delta x_1},$\n","\n","$\\frac{\\partial f}{\\partial x_2} = \\frac{f(x_1,x_2+\\delta x_2,x_3,...)-f(x_1,x_2-\\delta x_2,x_3,...)}{2\\delta x_2},$\n","\n","$...,$\n","\n","### and so on for each variable $x_i$. As a result, comoutation of numerical derivative is extremely inefficient for function with a large number of variables. But it is often used as a check to see if your autograd code is working correctly, or not.\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uFl_NFdGC_bq","colab":{"base_uri":"https://localhost:8080/","height":91},"executionInfo":{"status":"ok","timestamp":1599697105263,"user_tz":360,"elapsed":1009,"user":{"displayName":"Nilanjan Ray","photoUrl":"","userId":"13266317426275772701"}},"outputId":"682a3736-de16-4446-b1b5-28fb6d2f5678"},"source":["x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n","\n","def compute_f(x):\n","  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]+2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n","  return f\n","\n","f = compute_f(x)\n","\n","print(\"Vector variable x:\",x.data)\n","\n","print(\"Function f at x:\",f.data)\n","\n","# compute gradient of f at x\n","g = torch.autograd.grad(f,x)\n","\n","print(\"Exact gradient of f at x:\",g[0].data)\n","\n","num_g = torch.zeros(4)\n","h=1e-3\n","eye = torch.eye(4) # 4-by-4 identity matrix\n","for i in range(4):\n","  num_g[i] = compute_f(x+h*eye[i,:]) - compute_f(x-h*eye[i,:])\n","\n","num_g = num_g/(2.0*h)\n","\n","print(\"Numerical gradient of f at x:\",num_g.data)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Vector variable x: tensor([ 3., -1.,  0.,  1.])\n","Function f at x: tensor(215.)\n","Exact gradient of f at x: tensor([ 306., -144.,  -18., -310.])\n","Numerical gradient of f at x: tensor([ 305.9769, -143.9972,  -18.0054, -309.9976])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ELtLy6VZ7dGb","colab_type":"text"},"source":["# PyTorch autograd has no problem working with branches!"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TEQYs6zvrJ0P","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1599697342428,"user_tz":360,"elapsed":1101,"user":{"displayName":"Nilanjan Ray","photoUrl":"","userId":"13266317426275772701"}},"outputId":"1c261779-b5eb-4941-e76d-707821b54f83"},"source":["def compute_func(x):\n","  if x[0]<=0 and x[1]>=2:\n","    return x[0]**2 + x[1]**2Positive wh\n","  else:\n","    return -2.0*x[0]\n","\n","x = Variable(torch.tensor([-1.0,3.0]),requires_grad=True)\n","\n","f = compute_func(x)\n","g = torch.autograd.grad(f,x)\n","print(g)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["(tensor([-2.,  6.]),)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"DSsTsVXL7dGh","colab_type":"text"},"source":["# Autograd can work with loops too!\n","\n","### Here is an example. The function (due to Kepler https://en.wikipedia.org/wiki/Kepler%27s_equation#Inverse_Kepler_equation) below\n","\n","$y = f(x) = x - 0.2sin(x)$\n","\n","### has no closed form solution for its inverse $x = f^{-1}(y)$\n","\n","### We can compute an approximate inverse $x = g(y)$ by an iterative method\n","\n","### From calculus we know that $\\frac{df^{-1}}{dy}= \\frac{1}{\\frac{df}{dx}}$\n","\n","### So, derivative of $g$ computed by autograd can be verifed against calculus computed derivative: $\\frac{dg}{dy}\\approx \\frac{1}{\\frac{df}{dx}}$.\n","\n"]},{"cell_type":"code","metadata":{"id":"U4KszfRc7dGi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"status":"ok","timestamp":1599698108127,"user_tz":360,"elapsed":590,"user":{"displayName":"Nilanjan Ray","photoUrl":"","userId":"13266317426275772701"}},"outputId":"b9e0ff95-c3bc-43ca-f371-b7f8b0bafe23"},"source":["def compute_Kepler(x):\n","    y = x - 0.9*torch.sin(x)\n","    return y\n","\n","# Apply Newton's method to compute inverse\n","def compute_invKepler(y):\n","    x = y.detach().clone()\n","    for i in range(100):\n","        #x = x - (x-0.9*torch.sin(x)-y)/(1-0.9*torch.cos(y))\n","        x = y + 0.9*torch.sin(x) # fixed point iteration\n","    return x\n","\n","x = Variable(torch.tensor([20.]),requires_grad=True)\n","y = compute_Kepler(x)\n","print(\"x:\",x.item(),\"y:\",y.item())\n","\n","x_ = compute_invKepler(y)\n","\n","print(\"y:\",y.item(),\"x_:\",x_.item())\n","print(\"So, inverse apprximation is fine.\")\n","\n","# compute derivative of x at y using autograd on inverse\n","dx_ = torch.autograd.grad(x_,y) # Note that this autograd has to take care of the for loop\n","print(\"dx_/dy by Autograd that needs to go through the for loop:\",dx_[0])\n","\n","# compute derivative of x at y\n","dy_dx = torch.autograd.grad(y,x) # this can also be computed using math formula: f'(x) = 1 - 0.5cos(x)\n","print(\"dx/dy by Calculus:\",1./dy_dx[0])\n","\n"],"execution_count":15,"outputs":[{"output_type":"stream","text":["x: 20.0 y: 19.178348541259766\n","y: 19.178348541259766 x_: 19.999998092651367\n","So, inverse apprximation is fine.\n","dx_/dy by Autograd that needs to go through the for loop: tensor([1.5805])\n","dx/dy by Calculus: tensor([1.5805])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bS7HQGD07dGp","colab_type":"text"},"source":["# Now that we know Calculs works(!), can we write our own derivative instead of using autograd?\n","\n","## By the way, what might be an advantage of writing your own derivative here?"]},{"cell_type":"code","metadata":{"id":"oOL5PIw07dGq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1599698364928,"user_tz":360,"elapsed":626,"user":{"displayName":"Nilanjan Ray","photoUrl":"","userId":"13266317426275772701"}},"outputId":"3abd6d22-296a-4f02-d2c4-45f4daf8906b"},"source":["# Inherit from Function\n","class My_invKepler(Function):\n","\n","    # Note that both forward and backward are @staticmethods\n","    @staticmethod\n","    def forward(ctx, y):\n","        x = y\n","        for i in range(50):\n","            #x = x - (x-0.9*torch.sin(x)-y)/(1-0.9*torch.cos(y))\n","            x = y + 0.9*torch.sin(x)\n","        # Save for derivative computation\n","        ctx.save_for_backward(x,y)\n","        return x\n","\n","    @staticmethod\n","    def backward(ctx, output_grad):\n","        # retrieve saved tensors and use them in derivative calculation\n","        x, y = ctx.saved_tensors\n","\n","        # Return Jacobian-vector product (chain rule) - more to say about chain later!\n","        input_grad = (1.0/(1.0-0.9*torch.cos(x)))*output_grad\n","        #print(\"Output grad\",output_grad)\n","        return input_grad\n","    \n","x__ = My_invKepler.apply(y)\n","print(\"y:\",y.data,\"x__:\",x__.data)\n","\n","dx__ = torch.autograd.grad(x__,y)\n","print(dx__)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["y: tensor([19.1783]) x__: tensor([20.0000])\n","(tensor([1.5805]),)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KB85808hjuOK"},"source":["# Gradient descent"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PYaX2d0PYmOu","colab":{"base_uri":"https://localhost:8080/","height":203},"executionInfo":{"status":"ok","timestamp":1599699321437,"user_tz":360,"elapsed":1211,"user":{"displayName":"Nilanjan Ray","photoUrl":"","userId":"13266317426275772701"}},"outputId":"d20837b1-3d5f-4a51-e1b3-2d0b87cd8a88"},"source":["x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n","  \n","steplength = 1e-3 # for gradient descent\n","for i in range(1000):\n","  # function\n","  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]+2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n","  # compute grdaient\n","  g = torch.autograd.grad(f,x)\n","  # adjust variable\n","  x = x - steplength*g[0]\n","  if i%100==0:\n","    print(\"Current variable value:\",x.detach().numpy(),\"Current function value:\", f.item())\n"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Current variable value: [ 2.694 -0.856  0.018  1.31 ] Current function value: 215.0\n","Current variable value: [ 1.7774506  -0.19877124  0.5913045   1.3086869 ] Current function value: 4.060643672943115\n","Current variable value: [ 1.4662572  -0.16261245  0.5297499   1.0466366 ] Current function value: 2.330897808074951\n","Current variable value: [ 1.2376873  -0.13571279  0.47490963  0.85692835] Current function value: 1.4002106189727783\n","Current variable value: [ 1.0653068  -0.11562601  0.4296491   0.7175411 ] Current function value: 0.8787093758583069\n","Current variable value: [ 0.9327728  -0.10033523  0.39197212  0.6131387 ] Current function value: 0.574644923210144\n","Current variable value: [ 0.82897025 -0.08847379  0.36032715  0.5334208 ] Current function value: 0.3903746008872986\n","Current variable value: [ 0.7462303  -0.079105    0.3335184   0.47140023] Current function value: 0.2745198607444763\n","Current variable value: [ 0.6791866  -0.07157812  0.31061673  0.42227766] Current function value: 0.19912949204444885\n","Current variable value: [ 0.6240316  -0.06543481  0.29089484  0.38271144] Current function value: 0.14848771691322327\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"itsqyBm0j22X"},"source":["# Gradient descent using PyTorch's optmization"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HfW8cdRDiZYz","colab":{"base_uri":"https://localhost:8080/","height":223},"outputId":"78a38be2-55f0-4e8d-bccf-92a0f1536e4b"},"source":["x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n","  \n","optimizer = torch.optim.SGD([x], lr=1e-3, momentum=0.9) # create an optimizer that will do gradient descent optimization\n","\n","for i in range(100):\n","  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]+2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n","  optimizer.zero_grad()\n","  f.backward()\n","  optimizer.step()\n","  if i%10==0:\n","    print(\"Current variable value:\",x.detach().numpy(),\"Current function value:\", f.item())\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Current variable value: [ 2.694 -0.856  0.018  1.31 ] Current function value: 215.0\n","Current variable value: [ 1.549712   -0.2964205   0.73650396  1.6820586 ] Current function value: 11.441824913024902\n","Current variable value: [ 1.408362   -0.00583249  0.57971793  1.0532441 ] Current function value: 6.6437225341796875\n","Current variable value: [ 0.6763986  -0.03913701  0.24533094  1.1681743 ] Current function value: 5.709615707397461\n","Current variable value: [ 0.5996511  -0.13040473  0.5036107   0.53555894] Current function value: 1.093517541885376\n","Current variable value: [ 0.59016085 -0.07009609  0.31826916  0.27739534] Current function value: 0.27947038412094116\n","Current variable value: [ 0.5269876  -0.03624894  0.12591662  0.26216415] Current function value: 0.16900770366191864\n","Current variable value: [ 0.45544916 -0.04479674  0.13050896  0.22580846] Current function value: 0.08700942993164062\n","Current variable value: [ 0.399531   -0.04679754  0.18029977  0.18939461] Current function value: 0.034884385764598846\n","Current variable value: [ 0.35905084 -0.03668797  0.18895635  0.19141704] Current function value: 0.022943012416362762\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"l9OZiFkLnNhT"},"source":["# Chain rule of derivative"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"80oOHqfynMjr","colab":{"base_uri":"https://localhost:8080/","height":166},"executionInfo":{"status":"ok","timestamp":1599700274633,"user_tz":360,"elapsed":758,"user":{"displayName":"Nilanjan Ray","photoUrl":"","userId":"13266317426275772701"}},"outputId":"3a25a521-c160-4f6c-8194-b7fd18ed5f34"},"source":["z = Variable(torch.tensor([1.0,-1.0]),requires_grad=True)\n","\n","print(\"Variable z:\",z.data)\n","\n","def compute_x(z):\n","  x = torch.zeros(4)\n","  x[0] = z[0] - z[1]\n","  x[1] = z[0]**2\n","  x[2] = z[1]**2\n","  x[3] = z[0]**2+z[0]*z[1]\n","  return x\n","\n","x = compute_x(z)\n","print(\"function x:\",x.data)\n","\n","def compute_f(x):\n","  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]+2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n","  return f\n","\n","f = compute_f(x)\n","print(\"function f:\",f.item())\n","print(\"\")\n","# \n","# Let's compute gradient of f with respect to x\n","g_x = torch.autograd.grad(f,x,retain_graph=True,create_graph=True)\n","print(\"Gradient of f with respect x:\",g_x[0].data)\n","print(\"\")\n","# Now compute Jacobian of x with respect to z and multiply with g_x to use chain rule\n","g_z = torch.autograd.grad(x,z,g_x,retain_graph=True) \n","\n","# But PyTorch can compute derivative of f with respect to z directly - this is the amazing capability!\n","g = torch.autograd.grad(f,z)\n","\n","print(\"Gradient of f with respec to z by two-step calculation:\",g_z[0].data)\n","print(\"Gradient of f with respec to z by one-step calculation:\",g[0].data)"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Variable z: tensor([ 1., -1.])\n","function x: tensor([2., 1., 1., 0.])\n","function f: 390.0\n","\n","Gradient of f with respect x: tensor([ 344.,  348.,  226., -330.])\n","\n","Gradient of f with respec to z by two-step calculation: tensor([  710., -1126.])\n","Gradient of f with respec to z by one-step calculation: tensor([  710., -1126.])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NxQVN9qN7dG_","colab_type":"text"},"source":["## Let's verify the result using math formula\n","\n","$z_1 = 1, z_2 = -1$\n","\n","$x_1 = z_1 - z_2 = 2, x_2 = z_1^2 = 1, x_3 = z_2^2 = 1, x_4 = z_1^2+z_1z_2 = 0$\n","\n","## Let's compute partial derivative of $f$ with respect to $x_i$:\n","\n","$\\frac{\\partial f}{\\partial x_1} = 2(x_1+10x_2)+40(x_1-x_4)^3 = 2(2+10)+40(2-0)^3 = 24+320 = 344$\n","\n","$\\frac{\\partial f}{\\partial x_2} = 20(x_1+10x_2)+4(x_2+2x_3)^3 = 20(2+10)+4(1+2)^3 = 348$\n","\n","$\\frac{\\partial f}{\\partial x_3} = 10(x_3-x_4)+8(x_2+2x_3)^3 = 10(1)+8(3)^3 = 10+216 = 226$\n","\n","$\\frac{\\partial f}{\\partial x_4} = -10(x_3-x_4)-40(x_1-x_4)^3 = -10(1)-40(2)^3 = -10-320 = -330$\n","\n","## Let's now compute partial derivative of $x_i$ with respect to $z_1$:\n","\n","$\\frac{\\partial x_1}{\\partial z_1} = 1$\n","\n","$\\frac{\\partial x_2}{\\partial z_1} = 2z_1 = 2(1) = 2$\n","\n","$\\frac{\\partial x_3}{\\partial z_1} = 0$\n","\n","$\\frac{\\partial x_4}{\\partial z_1} = 2z_1 + z_2 = 2(1)-1 = 1$\n","\n","## Let's also compute partial derivative of $x_i$ with respect to $z_2$:\n","\n","$\\frac{\\partial x_1}{\\partial z_2} = -1$\n","\n","$\\frac{\\partial x_2}{\\partial z_2} = 0$\n","\n","$\\frac{\\partial x_3}{\\partial z_2} = 2z_2 = -2$\n","\n","$\\frac{\\partial x_4}{\\partial z_2} = z_1 = 1$\n","\n","## Now we can apply the chain rule of derivative:\n","\n","$\\frac{\\partial f}{\\partial z_1} = \\sum_i \\frac{\\partial f}{\\partial x_i}\\frac{\\partial x_i}{\\partial z_1} = 344(1) + 348(2) + 226(0) -330(1) = 710$\n","\n","$\\frac{\\partial f}{\\partial z_2} = \\sum_i \\frac{\\partial f}{\\partial x_i}\\frac{\\partial x_i}{\\partial z_2} = 344(-1) + 348(0) + 226(-2) -330(1) = -1126$\n","\n","## Using Jacobian notation, chain can be written as follows:\n","\n","\\begin{equation}\n","\\nabla_z f = (J_z^x) (\\nabla_x f) = \n","    \\begin{bmatrix}\n","        \\frac{\\partial x_1}{\\partial z_1} & \\frac{\\partial x_2}{\\partial z_1} & \\frac{\\partial x_3}{\\partial z_1} & \\frac{\\partial x_4}{\\partial z_1}\\\\\n","        \\frac{\\partial x_1}{\\partial z_2} & \\frac{\\partial x_2}{\\partial z_2} & \\frac{\\partial x_3}{\\partial z_2} & \\frac{\\partial x_4}{\\partial z_2}\n","    \\end{bmatrix}\n","    \\begin{bmatrix}\n","        \\frac{\\partial f}{\\partial x_1}\\\\\n","        \\frac{\\partial f}{\\partial x_2}\\\\\n","        \\frac{\\partial f}{\\partial x_3}\\\\\n","        \\frac{\\partial f}{\\partial x_4}\n","    \\end{bmatrix} =\n","    \\begin{bmatrix}\n","        1 & 2 & 0 & 1\\\\\n","        -1 & 0 & -2 & 1\n","    \\end{bmatrix}    \n","    \\begin{bmatrix}\n","        344\\\\\n","        348\\\\\n","        226\\\\\n","        -330\n","    \\end{bmatrix} = \n","    \\begin{bmatrix}\n","        710\\\\\n","        -1126\n","    \\end{bmatrix}\n","\\end{equation}\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RyEZ0Qy_zIFN"},"source":["# Optimization by gradient descent"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"igcWXotzzWfz","colab":{"base_uri":"https://localhost:8080/","height":203},"outputId":"84983fa9-aa13-4331-8163-7a3296326664"},"source":["steplength = 1e-3 # for gradient descent\n","for i in range(1000):\n","  # function\n","  f = compute_f(compute_x(z))\n","  # Compute gradient of f with respect to z directly\n","  # PyTorch takes care of chain rule of derivatives\n","  g = torch.autograd.grad(f,z) \n","  # adjust variable\n","  z = z - steplength*g[0]\n","  if i%100==0:\n","    print(\"Current variable value:\",z.detach().numpy(),\"Current function value:\", f.item())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Current variable value: [0.28999996 0.12600005] Current function value: 390.0\n","Current variable value: [0.10041686 0.16472499] Current function value: 0.002078988589346409\n","Current variable value: [0.0918402  0.16432461] Current function value: 0.0010676763486117125\n","Current variable value: [0.08976527 0.1616178 ] Current function value: 0.0009482738096266985\n","Current variable value: [0.08847897 0.15887022] Current function value: 0.0008560288697481155\n","Current variable value: [0.08736441 0.15630378] Current function value: 0.0007776079582981765\n","Current variable value: [0.08633856 0.15392138] Current function value: 0.000710221123881638\n","Current variable value: [0.08538311 0.15170398] Current function value: 0.0006518377340398729\n","Current variable value: [0.08448897 0.14963281] Current function value: 0.0006008768104948103\n","Current variable value: [0.08364932 0.14769198] Current function value: 0.000556100916583091\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"P1MbO8BG0Gzk"},"source":["# And of course optimization using PyTorch's gradient descent optimization"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bVsIisPK0RCk","colab":{"base_uri":"https://localhost:8080/","height":203},"executionInfo":{"status":"ok","timestamp":1599701615950,"user_tz":360,"elapsed":598,"user":{"displayName":"Nilanjan Ray","photoUrl":"","userId":"13266317426275772701"}},"outputId":"bbc85f41-7b6e-410c-be80-ada8ad5a6159"},"source":["z = Variable(torch.tensor([1.0,-1.0]),requires_grad=True)\n","  \n","optimizer = torch.optim.SGD([z], lr=1e-4, momentum=0.9) # create an optimizer that will do gradient descent optimization\n","\n","for i in range(100):\n","  f = compute_f(compute_x(z))\n","  optimizer.zero_grad()\n","  f.backward()\n","  optimizer.step()\n","  if i%10==0:\n","    print(\"Current variable value:\",z.detach().numpy(),\"Current function value:\", f.item())\n"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Current variable value: [ 0.929      -0.88740003] Current function value: 390.0\n","Current variable value: [-0.19928937  0.4960087 ] Current function value: 0.9813485741615295\n","Current variable value: [-0.4438892  0.7940718] Current function value: 24.753379821777344\n","Current variable value: [-0.15691993  0.49109557] Current function value: 2.473496198654175\n","Current variable value: [0.00094474 0.29169223] Current function value: 0.23995013535022736\n","Current variable value: [0.0661319  0.20713682] Current function value: 0.024552151560783386\n","Current variable value: [0.0927659  0.17456606] Current function value: 0.001717826584354043\n","Current variable value: [0.10217324 0.16280487] Current function value: 0.0023682829923927784\n","Current variable value: [0.10384294 0.1590711 ] Current function value: 0.003257464850321412\n","Current variable value: [0.10240355 0.1583022 ] Current function value: 0.0029500178061425686\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ka6kH50P7dHK","colab_type":"text"},"source":["# Toy optimization example involving softmax and Huber loss function"]},{"cell_type":"code","metadata":{"id":"OOL4vJp87dHL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":110},"executionInfo":{"status":"ok","timestamp":1599702157079,"user_tz":360,"elapsed":618,"user":{"displayName":"Nilanjan Ray","photoUrl":"","userId":"13266317426275772701"}},"outputId":"0839d6b6-c5a0-4666-d3bb-baa0e32a84d4"},"source":["# A loss function measures distance between a predicted and a target tensor\n","# An implementation of Huber loss function is given below\n","# We will make use of this loss function in gradient descent optimization\n","\n","# https://en.wikipedia.org/wiki/Huber_loss\n","    \n","def Huber_Loss(input,delta):\n","  m = (torch.abs(input)<=delta).detach().float()\n","  output = torch.sum(0.5*m*input**2 + delta*(1.0-m)*(torch.abs(input)-0.5*delta))\n","  return output\n","\n","# Test Huber loss\n","a = torch.tensor([[0.3, 2.0, -3.1],[0.5, 9.2, 0.1]])\n","print(a.numpy())\n","ha = Huber_Loss(a,1.0)\n","print(ha.numpy())\n","\n","b = torch.tensor([0.3, 2.0])\n","print(b.numpy())\n","hb = Huber_Loss(b,1.0)\n","print(hb.numpy())"],"execution_count":26,"outputs":[{"output_type":"stream","text":["[[ 0.3  2.  -3.1]\n"," [ 0.5  9.2  0.1]]\n","12.974999\n","[0.3 2. ]\n","1.545\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Q9t3KLjV7dHO","colab_type":"text"},"source":["# Here's the toy optimization problem\n","\n","$min_{z_1,...,z_{10}} \\{Huber\\_Loss(p(z_1,...,z_{10})-y)\\}$,\n","\n","## where $y$ is a given 1-hot vector and each element of the vector $p$ is defined as:\n","\n","$p_i = \\frac{exp(z_i)}{exp(z_1)+...+exp(z_{10})}$\n","\n","## The above function is known as softmax function. Loosely speaking, it turns any vector into  a probability vector. "]},{"cell_type":"code","metadata":{"id":"R5ZTwm5E7dHP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":353},"executionInfo":{"status":"ok","timestamp":1599702498383,"user_tz":360,"elapsed":643,"user":{"displayName":"Nilanjan Ray","photoUrl":"","userId":"13266317426275772701"}},"outputId":"e85b8c97-c415-4515-9b77-2ff77b5c9e11"},"source":["y = torch.zeros(10)\n","y[2] = 1.0\n","print(\"Target 1-hot vector:\",y.numpy())\n","z = Variable(torch.randn(y.shape),requires_grad=True) # Note that we are randomly initializing the z vector\n","\n","optimizer = torch.optim.SGD([z], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n","\n","# gradient descent\n","def gradient_descent(z,optimizer,softmax,loss,y,nIter,nPrint):\n","  for i in range(nIter):\n","    p = softmax(z)\n","    f = loss(p-y,1.0)\n","    optimizer.zero_grad()\n","    f.backward()\n","    optimizer.step()\n","    if i%nPrint==0:\n","      with np.printoptions(precision=3, suppress=True):\n","        print(\"Iteration:\",i,\"Variable:\", z.detach().numpy(),\"Loss: %0.6f\" % f.item())\n","  return p,z\n","p,z = gradient_descent(z,optimizer,F.softmax,Huber_Loss,y,1000,100)\n","print(\"Optimum z:\",z.data)\n","print(\"p at optimum:\",p.data)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Target 1-hot vector: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","Iteration: 0 Variable: [ 1.727 -0.25   0.589  0.63   0.486  0.547  0.954  0.908  0.835  1.063] Loss: 0.488091\n","Iteration: 100 Variable: [ 0.444 -0.381  5.313  0.244  0.163  0.199  0.384  0.368  0.34   0.416] Loss: 0.001682\n","Iteration: 200 Variable: [ 0.41  -0.395  5.568  0.216  0.138  0.173  0.351  0.336  0.309  0.382] Loss: 0.000975\n","Iteration: 300 Variable: [ 0.388 -0.404  5.73   0.199  0.122  0.156  0.331  0.316  0.29   0.361] Loss: 0.000688\n","Iteration: 400 Variable: [ 0.372 -0.411  5.849  0.186  0.11   0.143  0.316  0.302  0.276  0.346] Loss: 0.000532\n","Iteration: 500 Variable: [ 0.359 -0.416  5.944  0.176  0.1    0.134  0.304  0.29   0.264  0.333] Loss: 0.000433\n","Iteration: 600 Variable: [ 0.349 -0.421  6.022  0.167  0.092  0.126  0.295  0.28   0.255  0.323] Loss: 0.000365\n","Iteration: 700 Variable: [ 0.34  -0.425  6.089  0.16   0.086  0.119  0.286  0.272  0.247  0.315] Loss: 0.000315\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:11: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  # This is added back by InteractiveShellApp.init_path()\n"],"name":"stderr"},{"output_type":"stream","text":["Iteration: 800 Variable: [ 0.332 -0.428  6.148  0.154  0.08   0.113  0.279  0.265  0.24   0.307] Loss: 0.000278\n","Iteration: 900 Variable: [ 0.326 -0.431  6.199  0.148  0.075  0.107  0.272  0.258  0.234  0.3  ] Loss: 0.000248\n","Optimum z: tensor([ 0.3194, -0.4336,  6.2456,  0.1429,  0.0700,  0.1023,  0.2666,  0.2528,\n","         0.2282,  0.2946])\n","p at optimum: tensor([0.0026, 0.0012, 0.9799, 0.0022, 0.0020, 0.0021, 0.0025, 0.0024, 0.0024,\n","        0.0026])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KTUnFP6tjemZ"},"source":["# Hessian computation (optional)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ChfNnPqrVTII","colab":{"base_uri":"https://localhost:8080/","height":91},"outputId":"e9e53eac-6dc7-4371-9f50-0f0c3d50af11"},"source":["x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n","f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n","\n","# PyTorch does not compute Hessian (second order derivatives) directly\n","# PyTorch can compute Jacobian vector product \n","# We can use Jacobian vector product to compute Hessian\n","\n","# Step 1: compute gradient\n","g = torch.autograd.grad(f,x,retain_graph=True,create_graph=True) # compute gradient with two important flags on\n","\n","eye = torch.eye(4)\n","\n","# Step 2: Use product of Jacobian of g and columns of identity matrix to compute hessian of f\n","H = torch.stack([torch.autograd.grad(g,x,eye[:,i],retain_graph=True)[0] for i in range(4)]) # hessian\n","\n","print(\"Hessian:\",H.data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Hessian: tensor([[ 482.,   20.,    0., -480.],\n","        [  20.,  212.,  -24.,    0.],\n","        [   0.,  -24.,   58.,  -10.],\n","        [-480.,    0.,  -10.,  490.]])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QDghJQA9kf6L"},"source":["# Newton's method (optional)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ynGB444bke1c","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"de509019-83cc-48a1-8ddc-8e374d0d3858"},"source":["# Newton's optimization for an example - Powell Function (https://www.cs.ccu.edu.tw/~wtchu/courses/2014s_OPT/Lectures/Chapter%209%20Newton%27s%20Method.pdf)\n","# Minimize Powell function: f(x1,x2,x3,x4) = (x1+10x2)^2 + 5(x3-x4)^2 + (x2-2x3)^4 + 10(x1-x4)^4\n","\n","x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n","f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n","\n","def LineSearch(x,d):\n","  minstep = 1.0\n","  minval=1e10\n","  for i in range(10):\n","    step = (i+1)/10.0\n","    xp = x.data + step*d.data\n","    fval = (xp[0]+10.0*xp[1])**2 + 5.0*(xp[2]-xp[3])**2 + (xp[1]-2.0*xp[2])**4 + 10.0*(xp[0]-xp[3])**4\n","    if fval < minval:\n","      minval = fval\n","      minstep = step\n","  return minstep\n","\n","eye = torch.eye(4)\n","\n","for itr in range(10):\n","  # Step 1: compute Newton direction d\n","  g = torch.autograd.grad(f,x,retain_graph=True,create_graph=True) # gradient\n","  H = torch.stack([torch.autograd.grad(g,x,eye[:,i],retain_graph=True)[0] for i in range(4)]) # hessian\n","  d = torch.solve(-g[0].unsqueeze(1), H)[0].t().squeeze() # solve Newton system\n","  \n","  # Step 2: update x with Newton direction d\n","  step_length = LineSearch(x,d)\n","  x.data += step_length*d.data # often step_length is set as 1.0\n","  print(x.data)\n","  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n","  print(f.data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([ 1.5873, -0.1587,  0.2540,  0.2540])\n","tensor(31.8025)\n","tensor([ 1.0582, -0.1058,  0.1693,  0.1693])\n","tensor(6.2820)\n","tensor([ 0.7055, -0.0705,  0.1129,  0.1129])\n","tensor(1.2409)\n","tensor([ 0.4703, -0.0470,  0.0752,  0.0752])\n","tensor(0.2451)\n","tensor([ 0.3135, -0.0314,  0.0502,  0.0502])\n","tensor(0.0484)\n","tensor([ 0.2090, -0.0209,  0.0334,  0.0334])\n","tensor(0.0096)\n","tensor([ 0.1394, -0.0139,  0.0223,  0.0223])\n","tensor(0.0019)\n","tensor([ 0.0929, -0.0093,  0.0149,  0.0149])\n","tensor(0.0004)\n","tensor([ 0.0619, -0.0062,  0.0099,  0.0099])\n","tensor(7.3712e-05)\n","tensor([ 0.0413, -0.0041,  0.0066,  0.0066])\n","tensor(1.4560e-05)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s9qccXCcknY8"},"source":["# Conjugate gradient method (optional)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"KLI5FGLeQxUO","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"4b7fadf2-98cf-427c-f5de-1a2001fe8aa2"},"source":["# Newton method using conjugate gradient (https://en.wikipedia.org/wiki/Conjugate_gradient_method)\n","# Hessian-vector product computation needs one autograd call per iteration inside CG iteration\n","# So this method never computes and stores the full hessian matrix\n","# CG solver might converge faster than other general linear equation solver\n","# Also, it seems to be more stable\n","\n","x = Variable(torch.tensor([3.0,-1.0,0.0,1.0]),requires_grad=True)\n","f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n","\n","for itr in range(10):\n","  # Step 1: compute Newton direction d by CG method\n","  g = torch.autograd.grad(f,x,retain_graph=True,create_graph=True) # gradient\n","  r = -g[0].data.clone()\n","  p = r.clone()\n","  d = torch.tensor([0.,0.,0.,0.])\n","  rsold = torch.sum(r**2)\n","  for cg_itr in range(6): # cg_itr should be slightly larger length of variable - here variable length is 4\n","    q = torch.autograd.grad(g,x,p,retain_graph=True)[0] # hessian-vector (Jacobian-vector) product computation by autograd\n","    alpha = rsold/torch.sum(p*q)\n","    d += alpha*p\n","    r += -alpha*q\n","    rsnew = torch.sum(r**2)\n","    if rsnew<1e-10:\n","      break\n","    p = r + (rsnew/rsold)*p\n","    rsold = rsnew\n","    \n","  # Step 2: update x with Newton direction d\n","  step_length = LineSearch(x,d)\n","  x.data += step_length*d.data # often step_length is set as 1.0\n","  print(x.data)\n","  f = (x[0]+10.0*x[1])**2 + 5.0*(x[2]-x[3])**2 + (x[1]-2.0*x[2])**4 + 10.0*(x[0]-x[3])**4\n","  print(f.data)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([ 1.5873, -0.1587,  0.2540,  0.2540])\n","tensor(31.8025)\n","tensor([ 1.0582, -0.1058,  0.1693,  0.1693])\n","tensor(6.2820)\n","tensor([ 0.7055, -0.0705,  0.1129,  0.1129])\n","tensor(1.2409)\n","tensor([ 0.4703, -0.0470,  0.0752,  0.0752])\n","tensor(0.2451)\n","tensor([ 0.3135, -0.0314,  0.0502,  0.0502])\n","tensor(0.0484)\n","tensor([ 0.2090, -0.0209,  0.0334,  0.0334])\n","tensor(0.0096)\n","tensor([ 0.1394, -0.0139,  0.0223,  0.0223])\n","tensor(0.0019)\n","tensor([ 0.0929, -0.0093,  0.0149,  0.0149])\n","tensor(0.0004)\n","tensor([ 0.0619, -0.0062,  0.0099,  0.0099])\n","tensor(7.3712e-05)\n","tensor([ 0.0413, -0.0041,  0.0066,  0.0066])\n","tensor(1.4561e-05)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Sy0qu2FX7dHg","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}