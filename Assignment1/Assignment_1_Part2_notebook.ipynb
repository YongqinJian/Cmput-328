{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment_1_Part2_notebook.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Hfn-Yiy1lp5l","colab_type":"text"},"source":["# Lab Assignment 1 - Part2 \n","## With this assignment you will get to know more about gradient descent optimization and writing your own functions with forward and backward (i.e., gradient) passes\n","## You need to complete all the tasks in this notebook in the lab and show you work to the TA. Edit only those portions in the cells where it asks you to do so!"]},{"cell_type":"code","metadata":{"id":"Zp3BetP-d6cB","colab_type":"code","colab":{}},"source":["import torch\n","from torch.autograd import Variable\n","from torch.autograd import Function\n","import torch.nn.functional as F\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usEoWHBnSJW9","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"MJpovSL8d_-l","colab_type":"text"},"source":["## Huber loss function\n","https://en.wikipedia.org/wiki/Huber_loss"]},{"cell_type":"code","metadata":{"id":"GTp4nNf9d-zg","colab_type":"code","colab":{}},"source":["# A loss function measures distance between a predicted and a target tensor\n","# An implementation of Huber loss function is given below\n","# We will make use of this loss function in gradient descent optimization\n","def Huber_Loss(input,delta):\n","  m = (torch.abs(input)<=delta).detach().float()\n","  output = torch.sum(0.5*m*input**2 + delta*(1.0-m)*(torch.abs(input)-0.5*delta))\n","  return output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zZoxXPadgk-O","colab_type":"text"},"source":["# Test Huber loss with a couple of different examples"]},{"cell_type":"code","metadata":{"id":"KYO_KmUQfmnm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":103},"executionInfo":{"status":"ok","timestamp":1600559497333,"user_tz":360,"elapsed":1430,"user":{"displayName":"菅泳钦","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisNkEPQ1MIyihCSP3C7W28PHOGAXAyhKjVek8T=s64","userId":"04446447163478496729"}},"outputId":"84e70180-afdb-4a27-c6a7-20707a2957db"},"source":["a = torch.tensor([[0.3, 2.0, -3.1],[0.5, 9.2, 0.1]])\n","print(a.numpy())\n","ha = Huber_Loss(a,1.0)\n","print(ha.numpy())\n","\n","b = torch.tensor([0.3, 2.0])\n","print(b.numpy())\n","hb = Huber_Loss(b,1.0)\n","print(hb.numpy())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[ 0.3  2.  -3.1]\n"," [ 0.5  9.2  0.1]]\n","12.974999\n","[0.3 2. ]\n","1.545\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"26wLACTj7FkG","colab_type":"text"},"source":["# Gradient descent code\n","## Study the following generic gradient descent optimization code.\n","## Huber loss f measures the distance between a probability vector `z` and target 1-hot vector `target`.\n","## When `f.backward` is called, PyTorch first computes $\\nabla_z f$ (gradient of `f` with respect to `z`), then by chain rule it computes $\\nabla_{var} f = J^{z}_{var} \\nabla_z f$, where $J^{z}_{var}$ is the Jacobian of `z` with respect to `var`.\n","## Next, `optimizer.step()` call adjusts the variable `var` in the opposite direction of $\\nabla_{var} f.$"]},{"cell_type":"code","metadata":{"id":"NLxQgQaD7Krq","colab_type":"code","colab":{}},"source":["def gradient_descent(var,optimizer,softmax,loss,target,nIter,nPrint):\n","  for i in range(nIter):\n","    z = softmax(var)\n","    f = loss(z-target,1.0)\n","    optimizer.zero_grad()\n","    f.backward()\n","    optimizer.step()\n","    if i%nPrint==0:\n","      with np.printoptions(precision=3, suppress=True):\n","        print(\"Iteration:\",i,\"Variable:\", z.detach().numpy(),\"Loss: %0.6f\" % f.item())\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5viWaJpSiDuN","colab_type":"text"},"source":["# Gradient descent with Huber Loss\n","## The following cell shows how `gradient_descent` function can be used.\n","## The cell first creates a target 1-hot vector `y`, where only the 3rd place is on.\n","## It also creates a variable `x` with random initialization and an optimizer.\n","## Learning rate and momentum has been set to 0.1 and 0.9, respectively.\n","## Then it calls `gradient_descent` function."]},{"cell_type":"code","metadata":{"id":"AzRgWv_NiIeQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":262},"executionInfo":{"status":"ok","timestamp":1600559497651,"user_tz":360,"elapsed":1702,"user":{"displayName":"菅泳钦","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisNkEPQ1MIyihCSP3C7W28PHOGAXAyhKjVek8T=s64","userId":"04446447163478496729"}},"outputId":"8d7a9838-6902-4b13-c9f6-c70e3a8266b5"},"source":["y = torch.zeros(10)\n","y[2] = 1.0\n","print(\"Target 1-hot vector:\",y.numpy())\n","x = Variable(torch.randn(y.shape),requires_grad=True)\n","\n","optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n","\n","gradient_descent(x,optimizer,F.softmax,Huber_Loss,y,1000,100)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Target 1-hot vector: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","Iteration: 0 Variable: [0.085 0.19  0.367 0.04  0.138 0.032 0.035 0.018 0.039 0.055] Loss: 0.235679\n","Iteration: 100 Variable: [0.008 0.01  0.946 0.005 0.01  0.004 0.004 0.002 0.005 0.006] Loss: 0.001622\n","Iteration: 200 Variable: [0.006 0.008 0.959 0.004 0.007 0.003 0.003 0.002 0.004 0.005] Loss: 0.000949\n","Iteration: 300 Variable: [0.005 0.007 0.966 0.003 0.006 0.003 0.003 0.002 0.003 0.004] Loss: 0.000671\n","Iteration: 400 Variable: [0.004 0.006 0.97  0.003 0.005 0.002 0.002 0.001 0.003 0.003] Loss: 0.000519\n","Iteration: 500 Variable: [0.004 0.005 0.973 0.002 0.005 0.002 0.002 0.001 0.002 0.003] Loss: 0.000423\n","Iteration: 600 Variable: [0.004 0.005 0.975 0.002 0.004 0.002 0.002 0.001 0.002 0.003] Loss: 0.000357\n","Iteration: 700 Variable: [0.003 0.004 0.977 0.002 0.004 0.002 0.002 0.001 0.002 0.003] Loss: 0.000309\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"],"name":"stderr"},{"output_type":"stream","text":["Iteration: 800 Variable: [0.003 0.004 0.978 0.002 0.004 0.002 0.002 0.001 0.002 0.002] Loss: 0.000272\n","Iteration: 900 Variable: [0.003 0.004 0.979 0.002 0.004 0.002 0.002 0.001 0.002 0.002] Loss: 0.000243\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EtIf2LRqvOph","colab_type":"text"},"source":["# <font color='red'>30% Weight:</font> In this markdown cell, using [math mode](https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html), write gradient of Huber loss function: $output = \\sum_i 0.5 m_i (input)^{2}_{i} + \\delta (1-m_i)(|input_i|-0.5 \\delta)$ with respect to $input.$ Treat $m_i$ to be independent of $input_i,$ because we replaced *if* control statement with $m_i.$\n","## Your solution : $\\frac{\\partial (output)}{\\partial (input)_i} = \\sum_i \\frac{\\delta(1-m_i)input_i}{|input_i|}+m_iinput_i$"]},{"cell_type":"code","metadata":{"id":"XbKc4omJUKR-","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ly8SaBQ-lXbg","colab_type":"text"},"source":["# <font color='red'>30% Weight:</font> Define your own (correct!) rule of differentiation for Huber loss function\n","## Edit indicated line in the cell below. Use the following formula. Do not use for/while/any loop in your solution.\n","## For this function,  chain rule (Jacobian-vector product) takes the following form: $\\frac{\\partial (cost)}{\\partial (input)_i} = \\frac{\\partial (output)}{\\partial (input)_i} \\frac{\\partial (cost)}{\\partial (output)}.$\n","# In the `backward` method below, $\\frac{\\partial (cost)}{\\partial (output)}$ is denoted by `output_grad` and the $i^{th}$ component of `input_grad` is symbolized by $\\frac{\\partial (cost)}{\\partial (input)_i}.$"]},{"cell_type":"code","metadata":{"id":"UX4zC76XlWr0","colab_type":"code","colab":{}},"source":["# Inherit from torch.autograd.Function\n","class My_Huber_Loss(Function):\n","\n","    # Note that both forward and backward are @staticmethods\n","    @staticmethod\n","    def forward(ctx, input, delta):\n","        m = (torch.abs(input)<=delta).float()\n","        ctx.save_for_backward(input,torch.tensor(m),torch.tensor(delta))\n","        output = torch.sum(0.5*m*input**2 + delta*(1.0-m)*(torch.abs(input)-0.5*delta))\n","        return output\n","\n","    @staticmethod\n","    def backward(ctx, output_grad):\n","        # retrieve saved tensors and use them in derivative calculation\n","        input, m, delta = ctx.saved_tensors\n","\n","        # Return Jacobian-vector product (chain rule)\n","        # For Huber loss function the Jacobian happens to be a diagonal matrix\n","        # Also, note that output_grad is a scalar, because forward function returns a scalar value\n","        input_grad = ((delta*(1-m)*input)/torch.abs(input) + m*input)#*output_grad # complete this line, do not use for loop\n","        # must return two gradients becuase forward function takes in two arguments\n","        #print(input_grad)\n","        return input_grad, None"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WkG5zXGZcgja","colab_type":"text"},"source":["#Gradient Descent on Your Own Huber Loss\n","## You should get almost identical results as before if your rule of differentation is correct!"]},{"cell_type":"code","metadata":{"id":"6DKnFDK0pPjF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":297},"executionInfo":{"status":"ok","timestamp":1600559497663,"user_tz":360,"elapsed":1662,"user":{"displayName":"菅泳钦","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisNkEPQ1MIyihCSP3C7W28PHOGAXAyhKjVek8T=s64","userId":"04446447163478496729"}},"outputId":"1aee5555-161c-49f6-949b-5f7dd9c9bb62"},"source":["y = torch.zeros(10)\n","y[2] = 1.0\n","print(\"Target:\",y.numpy())\n","x = Variable(torch.randn(y.shape),requires_grad=True)\n","\n","optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n","\n","gradient_descent(x,optimizer,F.softmax,My_Huber_Loss.apply,y,1000,100)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Target: [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n","Iteration: 0 Variable: [0.082 0.045 0.02  0.113 0.052 0.288 0.033 0.324 0.028 0.014] Loss: 0.587482\n","Iteration: 100 Variable: [0.009 0.007 0.934 0.009 0.008 0.009 0.006 0.009 0.005 0.003] Loss: 0.002432\n","Iteration: 200 Variable: [0.006 0.005 0.956 0.006 0.005 0.006 0.004 0.006 0.004 0.002] Loss: 0.001101\n","Iteration: 300 Variable: [0.005 0.004 0.963 0.005 0.004 0.005 0.003 0.005 0.003 0.002] Loss: 0.000747\n","Iteration: 400 Variable: [0.004 0.004 0.968 0.004 0.004 0.004 0.003 0.004 0.003 0.001] Loss: 0.000566\n","Iteration: 500 Variable: [0.004 0.003 0.972 0.004 0.003 0.004 0.003 0.004 0.002 0.001] Loss: 0.000455\n","Iteration: 600 Variable: [0.004 0.003 0.974 0.004 0.003 0.004 0.002 0.004 0.002 0.001] Loss: 0.000380\n","Iteration: 700 Variable: [0.003 0.003 0.976 0.003 0.003 0.003 0.002 0.003 0.002 0.001] Loss: 0.000326\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n","  This is separate from the ipykernel package so we can avoid doing imports until\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n"],"name":"stderr"},{"output_type":"stream","text":["Iteration: 800 Variable: [0.003 0.003 0.977 0.003 0.003 0.003 0.002 0.003 0.002 0.001] Loss: 0.000286\n","Iteration: 900 Variable: [0.003 0.002 0.979 0.003 0.003 0.003 0.002 0.003 0.002 0.001] Loss: 0.000254\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"k2K4Q7ePPdfy","colab_type":"text"},"source":["# <font color='red'>40% Weight:</font> Your own softmax with forward and backward functions\n","## Edit indicated line in the cell below. Use the following formula. Do not use for/while/any loop in your solution.\n","## The Jacobian-vector product (chain rule) takes the following form using summation sign: $\\frac{\\partial (cost)}{\\partial (input)_i} = \\sum_j \\frac{\\partial (output)_j}{\\partial (input)_i} \\frac{\\partial (cost)}{\\partial (output)_j}$\n","# Once again note that, in the `backward` method below, $i^{th}$ component of `input_grad` and $j^{th}$ component of `output_grad` are denoted by $\\frac{\\partial (cost)}{\\partial (input)_i}$ and $\\frac{\\partial (cost)}{\\partial (output)_j}$, respectively."]},{"cell_type":"code","metadata":{"id":"zn52-xK_PijV","colab_type":"code","colab":{}},"source":["# Inherit from Function\n","class My_softmax(Function):\n","\n","    # Note that both forward and backward are @staticmethods\n","    @staticmethod\n","    def forward(ctx, input):\n","        output = F.softmax(input,dim=0)\n","        ctx.save_for_backward(output) # this is the only tensor you will need to save for backward function\n","        return output\n","\n","    # This function has only a single output, so it gets only one gradient\n","    @staticmethod\n","    def backward(ctx, output_grad):\n","        output = ctx.saved_tensors[0]\n","        # retrieve saved tensors and use them in derivative calculation\n","        # return Jacobian-vecor product\n","        input_grad =   # Complete this line\n","\n","        return input_grad"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fcixVFs4cwHO","colab_type":"text"},"source":["# Gradient Descent on your own Huber Loss and your own softmax"]},{"cell_type":"code","metadata":{"id":"UejqQeb4RZk0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1600560757642,"user_tz":360,"elapsed":431,"user":{"displayName":"菅泳钦","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisNkEPQ1MIyihCSP3C7W28PHOGAXAyhKjVek8T=s64","userId":"04446447163478496729"}},"outputId":"cc59a754-cf5d-418c-d10a-b7720f611825"},"source":["y = torch.zeros(10)\n","y[2] = 1.0\n","print(y)\n","x = Variable(torch.randn(y.shape),requires_grad=True)\n","print(x)\n","\n","optimizer = torch.optim.SGD([x], lr=1e-1, momentum=0.9) # create an optimizer that will do gradient descent optimization\n","\n","gradient_descent(x,optimizer,My_softmax.apply,My_Huber_Loss.apply,y,1000,100)\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["tensor([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.])\n","tensor([ 1.1794, -0.6172,  0.3024, -0.2511,  1.0290,  0.7454, -0.5135,  0.0971,\n","        -0.0446,  0.0690], requires_grad=True)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n","  \n"],"name":"stderr"},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-95-f2bdc8004465>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# create an optimizer that will do gradient descent optimization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMy_softmax\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMy_Huber_Loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-53-254d30204d99>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(var, optimizer, softmax, loss, target, nIter, nPrint)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mnPrint\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Function My_softmaxBackward returned an invalid gradient at index 0 - got [] but expected shape compatible with [10]\nException raised from validate_outputs at /pytorch/torch/csrc/autograd/engine.cpp:602 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f9d56fc31e2 in /usr/local/lib/python3.6/dist-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x336b180 (0x7f9d91383180 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #2: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x3fd (0x7f9d913883fd in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #3: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f9d91389fa1 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #4: torch::autograd::Engine::execute_with_graph_task(std::shared_ptr<torch::autograd::GraphTask> const&, std::shared_ptr<torch::autograd::Node>) + 0x37c (0x7f9d913876bc in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #5: torch::autograd::python::PythonEngine::execute_with_graph_task(std::shared_ptr<torch::autograd::GraphTask> const&, std::shared_ptr<torch::autograd::Node>) + 0x3c (0x7f9d9eb2276c in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_python.so)\nframe #6: torch::autograd::Engine::execute(std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, bool, bool, std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&) + 0x803 (0x7f9d913869f3 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_cpu.so)\nframe #7: torch::autograd::python::PythonEngine::execute(std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&, std::vector<at::Tensor, std::allocator<at::Tensor> > const&, bool, bool, std::vector<torch::autograd::Edge, std::allocator<torch::autograd::Edge> > const&) + 0x4e (0x7f9d9eb2256e in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_python.so)\nframe #8: THPEngine_run_backward(THPEngine*, _object*, _object*) + 0xa54 (0x7f9d9eb23254 in /usr/local/lib/python3.6/dist-packages/torch/lib/libtorch_python.so)\nframe #9: /usr/bin/python3() [0x50a7f5]\nframe #10: _PyEval_EvalFrameDefault + 0x1226 (0x50cfd6 in /usr/bin/python3)\nframe #11: /usr/bin/python3() [0x507f24]\nframe #12: /usr/bin/python3() [0x509c50]\nframe #13: /usr/bin/python3() [0x50a64d]\nframe #14: _PyEval_EvalFrameDefault + 0x444 (0x50c1f4 in /usr/bin/python3)\nframe #15: /usr/bin/python3() [0x507f24]\nframe #16: /usr/bin/python3() [0x509c50]\nframe #17: /usr/bin/python3() [0x50a64d]\nframe #18: _PyEval_EvalFrameDefault + 0x444 (0x50c1f4 in /usr/bin/python3)\nframe #19: /usr/bin/python3() [0x509918]\nframe #20: /usr/bin/python3() [0x50a64d]\nframe #21: _PyEval_EvalFrameDefault + 0x444 (0x50c1f4 in /usr/bin/python3)\nframe #22: /usr/bin/python3() [0x507f24]\nframe #23: /usr/bin/python3() [0x5165a5]\nframe #24: /usr/bin/python3() [0x50a47f]\nframe #25: _PyEval_EvalFrameDefault + 0x444 (0x50c1f4 in /usr/bin/python3)\nframe #26: /usr/bin/python3() [0x507f24]\nframe #27: /usr/bin/python3() [0x509c50]\nframe #28: /usr/bin/python3() [0x50a64d]\nframe #29: _PyEval_EvalFrameDefault + 0x444 (0x50c1f4 in /usr/bin/python3)\nframe #30: /usr/bin/python3() [0x507f24]\nframe #31: /usr/bin/python3() [0x509c50]\nframe #32: /usr/bin/python3() [0x50a64d]\nframe #33: _PyEval_EvalFrameDefault + 0x1226 (0x50cfd6 in /usr/bin/python3)\nframe #34: /usr/bin/python3() [0x507f24]\nframe #35: _PyFunction_FastCallDict + 0x2e2 (0x509202 in /usr/bin/python3)\nframe #36: /usr/bin/python3() [0x594b01]\nframe #37: PyObject_Call + 0x3e (0x59fe1e in /usr/bin/python3)\nframe #38: _PyEval_EvalFrameDefault + 0x17e6 (0x50d596 in /usr/bin/python3)\nframe #39: /usr/bin/python3() [0x507f24]\nframe #40: /usr/bin/python3() [0x509c50]\nframe #41: /usr/bin/python3() [0x50a64d]\nframe #42: _PyEval_EvalFrameDefault + 0x1226 (0x50cfd6 in /usr/bin/python3)\nframe #43: /usr/bin/python3() [0x507f24]\nframe #44: /usr/bin/python3() [0x509c50]\nframe #45: /usr/bin/python3() [0x50a64d]\nframe #46: _PyEval_EvalFrameDefault + 0x444 (0x50c1f4 in /usr/bin/python3)\nframe #47: /usr/bin/python3() [0x509918]\nframe #48: /usr/bin/python3() [0x50a64d]\nframe #49: _PyEval_EvalFrameDefault + 0x444 (0x50c1f4 in /usr/bin/python3)\nframe #50: /usr/bin/python3() [0x509918]\nframe #51: /usr/bin/python3() [0x50a64d]\nframe #52: _PyEval_EvalFrameDefault + 0x444 (0x50c1f4 in /usr/bin/python3)\nframe #53: /usr/bin/python3() [0x507f24]\nframe #54: /usr/bin/python3() [0x588e91]\nframe #55: PyObject_Call + 0x3e (0x59fe1e in /usr/bin/python3)\nframe #56: _PyEval_EvalFrameDefault + 0x17e6 (0x50d596 in /usr/bin/python3)\nframe #57: /usr/bin/python3() [0x507f24]\nframe #58: /usr/bin/python3() [0x588e91]\nframe #59: PyObject_Call + 0x3e (0x59fe1e in /usr/bin/python3)\nframe #60: _PyEval_EvalFrameDefault + 0x17e6 (0x50d596 in /usr/bin/python3)\nframe #61: /usr/bin/python3() [0x507f24]\nframe #62: /usr/bin/python3() [0x509c50]\nframe #63: /usr/bin/python3() [0x50a64d]\n"]}]}]}