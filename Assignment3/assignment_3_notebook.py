# -*- coding: utf-8 -*-
"""Assignment_3_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18-MTDRNuqWbjaHmFNxqcR-Shyy1gu8Ow

**Import** *and* setup some auxiliary functions
"""

# Don't edit this cell
import os
import timeit
import time
import numpy as np
from collections import OrderedDict
from pprint import pformat
from tqdm import tqdm
from google.colab import drive

import torch
import torch.nn as nn
import torch.nn.init as init
import torch.nn.functional as F
import torch.backends.cudnn as cudnn
from torch.utils.data.sampler import *
from torchvision import transforms, datasets

torch.multiprocessing.set_sharing_strategy('file_system')
cudnn.benchmark = True

# TODO: Main model definition + any utilities such as weight initialization or custom layers, ADD DROPOUT, BATCHNORM, SKIP CONNECTION,
class Net(nn.Module):
  def __init__(self):
    super(Net,self).__init__()
    # *** Define my cnn layers
    self.conv1 = nn.Conv2d(3,32,kernel_size = 3,padding=1)
    self.conv2 = nn.Conv2d(32,32,kernel_size = 3,padding=1)
    self.conv3 = nn.Conv2d(32,64,kernel_size = 3)
    self.conv4 = nn.Conv2d(64,128,kernel_size = 3)

    # *** 3 batch norm defined, but bm3 is used twice as 2nd conv layer doesnot change the depth
    self.bm1 = nn.BatchNorm2d(32)
    self.bm2 = nn.BatchNorm2d(64)
    self.bm3 = nn.BatchNorm2d(128)

    self.drop = nn.Dropout2d(p=0.2) # *** Requirment f dropout rate = 0.2

    # *** fully connected layers
    self.fc1 = nn.Linear(4608,1024)
    self.fc2 = nn.Linear(1024,256)
    self.fc3 = nn.Linear(256,10)


  def forward(self,x):
    # *** first conv layer block *** #

    sk1 = x # store input tensors in use of skip connection later
    sk1 = self.conv1(x)
    x = F.relu(self.bm1(self.conv1(x))) # batch normalization is used in every conv layer
    x = self.bm1(self.conv2(x)+ sk1) # also used skip connecton with var 'sk1'
    x = F.relu(x)
    x = F.max_pool2d(x,kernel_size = 2,stride = 2) # down sampling
    x = self.drop(x)  # apply drop out after down sampling

    # *** second conv layer block *** #
    sk1 = x
    sk1 = self.conv4(self.conv3(sk1))
    x = F.relu(self.bm2(self.conv3(x)))
    x = F.relu(self.bm3(self.conv4(x)+sk1))
    x = F.max_pool2d(x,kernel_size = 2,stride = 2)
    x = self.drop(x)
    

    # *** fully connected layer block *** #
    x = x.view(x.size(0),-1)
    
    x = F.relu(self.fc1(x))
    x = F.relu(self.fc2(x))
    x = self.drop(x)
    x = self.fc3(x)

    return F.log_softmax(x)

# TODO: Cifar-10 dataloading
def load_data(config):
    """
    Load cifar-10 dataset using torchvision, take the last 5k of the training data to be validation data
    """
    transform_train = transforms.Compose([
        transforms.RandomCrop(32, padding=4),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    transform_test = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),
    ])

    # *** Loading data and taking the last 5k training set as validation set #
    Cifar_training = datasets.CIFAR10(root='./data',train=True,download=True,transform = transform_train)
    Cifar_testing = datasets.CIFAR10(root='./data',train=False,download=True,transform = transform_test)

    indices = np.arange(0,50000)  # split training set
    train_set = torch.utils.data.Subset(Cifar_training,indices[:45000])
    valid_set = torch.utils.data.Subset(Cifar_training,indices[45000:])

    batch_size_train = config['batch_size']
    train_dataloader = torch.utils.data.DataLoader(train_set,batch_size = batch_size_train, shuffle=True)
    valid_dataloader = torch.utils.data.DataLoader(valid_set,batch_size = batch_size_train, shuffle=True)
    test_dataloader = torch.utils.data.DataLoader(Cifar_testing,batch_size = 1,shuffle = True)
  
    return train_dataloader, valid_dataloader, test_dataloader

# TODO : Main trainig + validation, returns the final model, save your best checkpoint based on the best validation accuracy
def train(trainloader, valid_dataloader, device, config):
    # set up model and mount google drive
    network = Net().to(device)
    drive.mount('/content/gdrive', force_remount=True)

    # set up parameters for optimizer
    learning_rate = config['lr']
    mo = config['momentum']
    wd = config['regular_constant']
    optimizer = torch.optim.SGD(network.parameters(),lr = learning_rate,momentum=mo,weight_decay=wd) 
    # two optimizer are considered 
    #optimizer = torch.optim.Adam(network.parameters(),lr = learning_rate,weight_decay=wd) 

    valid_acc=[]  # this list is used for saving check point if current loss is smaller than other valid loss

    epochs = config['num_epochs'] # get number of epochs from config
    for epoch in range(epochs): # loop over the dataset several times
      #network = network.to(device)
      # train part
      network.train() # set model to train mode
      for batch_idx ,(data,target) in enumerate(trainloader):
        # The below part is pretty similar to other deep learning code
        data = data.to(device)
        target = target.to(device)
        optimizer.zero_grad()
        output = network(data)
        loss = F.nll_loss(output,target) # nll_loss function is used here
        loss.backward()
        optimizer.step()

      # *** validation set, validate for every epoch and store valid loss for check point use
      network.eval()
      valid_loss=0
      with torch.no_grad():
        for data,target in valid_dataloader:
          data = data.to(device)
          target = target.to(device)
          output = network(data)
          valid_loss += F.nll_loss(output,target,size_average=False).item()
      valid_loss /= len(valid_dataloader.dataset)
      valid_acc.append(valid_loss)

      # if the current valid loss is smaller than any other epoch, then save current model as check point
      if valid_loss == min(valid_acc):
        torch.save(network.state_dict(),'gdrive/My Drive/checkpoint/ckpt.pth')

    return network

def save_model_colab_for_submission(model):  # if you are running on colab
  drive.mount('/content/gdrive/', force_remount=True)
  torch.save(model.to(torch.device("cpu")), 'gdrive/My Drive/model.pt') # you will find the model in your home drive
  
def save_model_local_for_submission(model):  # if you are running on your local machine
  torch.save(model.to(torch.device("cpu")), 'model.pt')

#TODO: Implement testing
def test(net, testloader, device):
  # test model that trained before, no much difference compares to prior assignments
    correct = 0
    total = 10000
    net.eval()
    with torch.no_grad():
      for data,target in testloader:
        data = data.to(device)
        target = target.to(device)
        output=net(data)
        pred = output.data.max(1,keepdim=True)[1]
        correct += pred.eq(target.data.view_as(pred)).sum()

    
    ###
    correct = correct.cpu().numpy()
    return 100.*correct/total, correct, total

def run():
  # set parameters cifar10
  config = {
        'lr': 0.03,
        'num_epochs': 40,
        'batch_size': 1000,
        'num_classes': 10,
        'momentum':0.9,
        'regular_constant': 0.001,
       }
    
  device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
  
  train_dataloader, valid_dataloader, test_dataloader = load_data(config)
  
  model = train(train_dataloader, valid_dataloader, device, config)
  
  # Testing and saving for submission
  device = torch.device("cpu")
  
  # *******************************************************************************************#
  # *** I have earse the following line as I assume checkpoint directory exist ***#
  # *** Meanwhile I had some trouble with the os.path/directory setting, so I  ***#
  # *** modified torch.load path to directly load my checkpoint. For grading   ***#
  # *** or relevent reason, please change thoes line to suit your need! Thank you*#
  # *******************************************************************************************#

  #assert os.path.isdir('checkpoint'), 'Error: no checkpoint directory found!'
  checkpoint = torch.load('gdrive/My Drive/checkpoint/ckpt.pth')
  model.load_state_dict(checkpoint)
  model.eval()
  
  start_time = timeit.default_timer()
  test_acc, test_correct, test_total = test(model.to(device), test_dataloader, device)
  end_time = timeit.default_timer()
  test_time = (end_time - start_time)
  
  save_model_colab_for_submission(model)

  return test_acc, test_correct, test_time

"""Main loop. Run time and total score will be shown below."""

# Don't edit this cell
def compute_score(acc, min_thres=65, max_thres=80):
  # Your Score thresholds
  if acc <= min_thres:
      base_score = 0.0
  elif acc >= max_thres:
      base_score = 100.0
  else:
      base_score = float(acc - min_thres) / (max_thres - min_thres) * 100
  return base_score

def main():
    
    accuracy, correct, run_time = run()
    
    score = compute_score(accuracy)
    
    result = OrderedDict(correct=correct,
                         accuracy=accuracy,
                         run_time=run_time,
                         score=score)
    
    with open('result.txt', 'w') as f:
        f.writelines(pformat(result, indent=4))
    print("\nResult:\n", pformat(result, indent=4))


main()