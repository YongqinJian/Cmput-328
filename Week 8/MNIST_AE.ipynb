{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_AE.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4BNOnZe46CQ",
        "outputId": "a96b9417-96ef-4385-9f30-a8ee8bbb2f21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        }
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import random_split\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2uiYpfC4_aW"
      },
      "source": [
        "\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "\n",
        "MNIST_training = torchvision.datasets.MNIST('/content/drive/My Drive/CMPUT328_2020/Week08/MNIST_dataset/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize((0.0,), (1.0,))]))\n",
        "\n",
        "MNIST_test_set = torchvision.datasets.MNIST('/content/drive/My Drive/CMPUT328_2020/Week08//MNIST_dataset/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize((0.0,), (1.0,))]))\n",
        "\n",
        "# create a training and a validation set\n",
        "MNIST_training_set, MNIST_validation_set = random_split(MNIST_training, [55000, 5000])\n",
        "\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(MNIST_training_set,batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(MNIST_validation_set,batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(MNIST_test_set,batch_size=batch_size_test, shuffle=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PSq8BOG85GyN",
        "outputId": "8d9ce458-9d17-4d70-bcc2-80f2681c7cbd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "print(example_data.dtype)\n",
        "print(example_targets.dtype)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.float32\n",
            "torch.int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1hgLmOT5KsW",
        "outputId": "9493280d-8c6e-41e6-f182-61e34d1b98f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(example_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Ground Truth: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7dtmJYX2Pbp"
      },
      "source": [
        "# Fully Connected AutoEncoder Architecture\n",
        "class AE(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AE, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(28*28, 300, True)\n",
        "        self.fc2 = torch.nn.Linear(300, 20, True)\n",
        "        self.fc3 = torch.nn.Linear(20, 300, True)\n",
        "        self.fc4 = torch.nn.Linear(300,28*28, True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        x = torch.sigmoid(self.fc4(x))\n",
        "        x = x.view(x.size(0), 1, 28, 28)\n",
        "        return x\n",
        "\n",
        "    def code(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return x"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lcC3yk75s46H"
      },
      "source": [
        "#https://www.kaggle.com/ljlbarbosa/convolution-autoencoder-pytorch\n",
        "\n",
        "# define the NN architecture\n",
        "class ConvAutoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvAutoencoder, self).__init__()\n",
        "        ## encoder layers ##\n",
        "        # conv layer (depth from 1 --> 16), 3x3 kernels\n",
        "        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)  \n",
        "        # conv layer (depth from 16 --> 4), 3x3 kernels\n",
        "        self.conv2 = nn.Conv2d(16, 4, 3, padding=1)\n",
        "        # pooling layer to reduce x-y dims by two; kernel and stride of 2\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        \n",
        "        ## decoder layers ##\n",
        "        ## a kernel of 2 and a stride of 2 will increase the spatial dims by 2\n",
        "        self.t_conv1 = nn.ConvTranspose2d(4, 16, 2, stride=2)\n",
        "        self.t_conv2 = nn.ConvTranspose2d(16, 1, 2, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ## encode ##\n",
        "        # add hidden layers with relu activation function\n",
        "        # and maxpooling after\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = self.pool(x)\n",
        "        # add second hidden layer\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)  # compressed representation\n",
        "        \n",
        "        ## decode ##\n",
        "        # add transpose conv layers, with relu activation function\n",
        "        x = F.relu(self.t_conv1(x))\n",
        "        # output layer (with sigmoid for scaling from 0 to 1)\n",
        "        x = F.sigmoid(self.t_conv2(x))\n",
        "                \n",
        "        return x\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cy8SKSrG6KxV"
      },
      "source": [
        "n_epochs=10\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "validation_losses = []\n",
        "validation_counter = [i*len(train_loader.dataset) for i in range(n_epochs + 1)]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxV2ZI1V28ib"
      },
      "source": [
        "def train(epoch):\n",
        "  network.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = network(data)\n",
        "    loss = F.mse_loss(output, data)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if batch_idx % log_interval == 0:\n",
        "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "        100. * batch_idx / len(train_loader), loss.item()))\n",
        "      train_losses.append(loss.item())\n",
        "      train_counter.append(\n",
        "        (batch_idx*64) + ((epoch-1)*len(train_loader.dataset)))\n",
        "      torch.save(network.state_dict(), '/content/drive/My Drive/CMPUT328_2020/Week08/MNIST_NN_results/model.pth')\n",
        "      torch.save(optimizer.state_dict(), '/content/drive/My Drive/CMPUT328_2020/Week08/MNIST_NN_results/optimizer.pth')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8i_T2Gm2_4X"
      },
      "source": [
        "def validation():\n",
        "  network.eval()\n",
        "  validation_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in validation_loader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      output = network(data)\n",
        "      validation_loss += F.mse_loss(output, data).item()\n",
        "  validation_loss /= len(validation_loader.dataset)\n",
        "  validation_losses.append(validation_loss)\n",
        "  print('\\nValidation set: Avg. loss: {:.6f}\\n'.format(validation_loss))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1vYbfub3FGp"
      },
      "source": [
        "def test():\n",
        "  network.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for data, target in test_loader:\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      output = network(data)\n",
        "      test_loss += F.mse_loss(output, data).item()\n",
        "  test_loss /= len(test_loader.dataset)\n",
        "  print('\\nTest set: Avg. loss: {:.6f}\\n'.format(test_loss))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWbjRf283KGc",
        "outputId": "6fb8382e-63f1-4dcc-f45a-6960ea0d6cb7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "log_interval = 200\n",
        "learning_rate=0.001\n",
        "n_epochs = 100\n",
        "\n",
        "network = AE().to(device)\n",
        "#network = ConvAutoencoder().to(device)\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(network.parameters(), lr=learning_rate, amsgrad=True)\n",
        "\n",
        "validation()\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "  train(epoch)\n",
        "  validation()\n",
        "test()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Validation set: Avg. loss: 0.003665\n",
            "\n",
            "Train Epoch: 1 [0/55000 (0%)]\tLoss: 0.232224\n",
            "Train Epoch: 1 [12800/55000 (23%)]\tLoss: 0.038521\n",
            "Train Epoch: 1 [25600/55000 (47%)]\tLoss: 0.027927\n",
            "Train Epoch: 1 [38400/55000 (70%)]\tLoss: 0.024526\n",
            "Train Epoch: 1 [51200/55000 (93%)]\tLoss: 0.021713\n",
            "\n",
            "Validation set: Avg. loss: 0.000356\n",
            "\n",
            "Train Epoch: 2 [0/55000 (0%)]\tLoss: 0.022108\n",
            "Train Epoch: 2 [12800/55000 (23%)]\tLoss: 0.021529\n",
            "Train Epoch: 2 [25600/55000 (47%)]\tLoss: 0.020580\n",
            "Train Epoch: 2 [38400/55000 (70%)]\tLoss: 0.019456\n",
            "Train Epoch: 2 [51200/55000 (93%)]\tLoss: 0.021694\n",
            "\n",
            "Validation set: Avg. loss: 0.000310\n",
            "\n",
            "Train Epoch: 3 [0/55000 (0%)]\tLoss: 0.019505\n",
            "Train Epoch: 3 [12800/55000 (23%)]\tLoss: 0.017301\n",
            "Train Epoch: 3 [25600/55000 (47%)]\tLoss: 0.018536\n",
            "Train Epoch: 3 [38400/55000 (70%)]\tLoss: 0.018155\n",
            "Train Epoch: 3 [51200/55000 (93%)]\tLoss: 0.018747\n",
            "\n",
            "Validation set: Avg. loss: 0.000286\n",
            "\n",
            "Train Epoch: 4 [0/55000 (0%)]\tLoss: 0.019680\n",
            "Train Epoch: 4 [12800/55000 (23%)]\tLoss: 0.015860\n",
            "Train Epoch: 4 [25600/55000 (47%)]\tLoss: 0.016997\n",
            "Train Epoch: 4 [38400/55000 (70%)]\tLoss: 0.018289\n",
            "Train Epoch: 4 [51200/55000 (93%)]\tLoss: 0.018073\n",
            "\n",
            "Validation set: Avg. loss: 0.000272\n",
            "\n",
            "Train Epoch: 5 [0/55000 (0%)]\tLoss: 0.016464\n",
            "Train Epoch: 5 [12800/55000 (23%)]\tLoss: 0.018410\n",
            "Train Epoch: 5 [25600/55000 (47%)]\tLoss: 0.014733\n",
            "Train Epoch: 5 [38400/55000 (70%)]\tLoss: 0.018503\n",
            "Train Epoch: 5 [51200/55000 (93%)]\tLoss: 0.018284\n",
            "\n",
            "Validation set: Avg. loss: 0.000263\n",
            "\n",
            "Train Epoch: 6 [0/55000 (0%)]\tLoss: 0.017358\n",
            "Train Epoch: 6 [12800/55000 (23%)]\tLoss: 0.016487\n",
            "Train Epoch: 6 [25600/55000 (47%)]\tLoss: 0.018087\n",
            "Train Epoch: 6 [38400/55000 (70%)]\tLoss: 0.016467\n",
            "Train Epoch: 6 [51200/55000 (93%)]\tLoss: 0.016820\n",
            "\n",
            "Validation set: Avg. loss: 0.000257\n",
            "\n",
            "Train Epoch: 7 [0/55000 (0%)]\tLoss: 0.015888\n",
            "Train Epoch: 7 [12800/55000 (23%)]\tLoss: 0.017729\n",
            "Train Epoch: 7 [25600/55000 (47%)]\tLoss: 0.017621\n",
            "Train Epoch: 7 [38400/55000 (70%)]\tLoss: 0.014139\n",
            "Train Epoch: 7 [51200/55000 (93%)]\tLoss: 0.013106\n",
            "\n",
            "Validation set: Avg. loss: 0.000251\n",
            "\n",
            "Train Epoch: 8 [0/55000 (0%)]\tLoss: 0.016548\n",
            "Train Epoch: 8 [12800/55000 (23%)]\tLoss: 0.015773\n",
            "Train Epoch: 8 [25600/55000 (47%)]\tLoss: 0.016271\n",
            "Train Epoch: 8 [38400/55000 (70%)]\tLoss: 0.015401\n",
            "Train Epoch: 8 [51200/55000 (93%)]\tLoss: 0.017266\n",
            "\n",
            "Validation set: Avg. loss: 0.000245\n",
            "\n",
            "Train Epoch: 9 [0/55000 (0%)]\tLoss: 0.015111\n",
            "Train Epoch: 9 [12800/55000 (23%)]\tLoss: 0.015225\n",
            "Train Epoch: 9 [25600/55000 (47%)]\tLoss: 0.015215\n",
            "Train Epoch: 9 [38400/55000 (70%)]\tLoss: 0.015778\n",
            "Train Epoch: 9 [51200/55000 (93%)]\tLoss: 0.015716\n",
            "\n",
            "Validation set: Avg. loss: 0.000244\n",
            "\n",
            "Train Epoch: 10 [0/55000 (0%)]\tLoss: 0.016510\n",
            "Train Epoch: 10 [12800/55000 (23%)]\tLoss: 0.016273\n",
            "Train Epoch: 10 [25600/55000 (47%)]\tLoss: 0.016064\n",
            "Train Epoch: 10 [38400/55000 (70%)]\tLoss: 0.015231\n",
            "Train Epoch: 10 [51200/55000 (93%)]\tLoss: 0.018153\n",
            "\n",
            "Validation set: Avg. loss: 0.000239\n",
            "\n",
            "Train Epoch: 11 [0/55000 (0%)]\tLoss: 0.015105\n",
            "Train Epoch: 11 [12800/55000 (23%)]\tLoss: 0.014227\n",
            "Train Epoch: 11 [25600/55000 (47%)]\tLoss: 0.014042\n",
            "Train Epoch: 11 [38400/55000 (70%)]\tLoss: 0.015173\n",
            "Train Epoch: 11 [51200/55000 (93%)]\tLoss: 0.013143\n",
            "\n",
            "Validation set: Avg. loss: 0.000237\n",
            "\n",
            "Train Epoch: 12 [0/55000 (0%)]\tLoss: 0.016389\n",
            "Train Epoch: 12 [12800/55000 (23%)]\tLoss: 0.014842\n",
            "Train Epoch: 12 [25600/55000 (47%)]\tLoss: 0.016336\n",
            "Train Epoch: 12 [38400/55000 (70%)]\tLoss: 0.014670\n",
            "Train Epoch: 12 [51200/55000 (93%)]\tLoss: 0.013909\n",
            "\n",
            "Validation set: Avg. loss: 0.000234\n",
            "\n",
            "Train Epoch: 13 [0/55000 (0%)]\tLoss: 0.013545\n",
            "Train Epoch: 13 [12800/55000 (23%)]\tLoss: 0.012887\n",
            "Train Epoch: 13 [25600/55000 (47%)]\tLoss: 0.015823\n",
            "Train Epoch: 13 [38400/55000 (70%)]\tLoss: 0.015014\n",
            "Train Epoch: 13 [51200/55000 (93%)]\tLoss: 0.014065\n",
            "\n",
            "Validation set: Avg. loss: 0.000233\n",
            "\n",
            "Train Epoch: 14 [0/55000 (0%)]\tLoss: 0.014335\n",
            "Train Epoch: 14 [12800/55000 (23%)]\tLoss: 0.013056\n",
            "Train Epoch: 14 [25600/55000 (47%)]\tLoss: 0.017859\n",
            "Train Epoch: 14 [38400/55000 (70%)]\tLoss: 0.014160\n",
            "Train Epoch: 14 [51200/55000 (93%)]\tLoss: 0.012713\n",
            "\n",
            "Validation set: Avg. loss: 0.000229\n",
            "\n",
            "Train Epoch: 15 [0/55000 (0%)]\tLoss: 0.013810\n",
            "Train Epoch: 15 [12800/55000 (23%)]\tLoss: 0.013987\n",
            "Train Epoch: 15 [25600/55000 (47%)]\tLoss: 0.013694\n",
            "Train Epoch: 15 [38400/55000 (70%)]\tLoss: 0.013792\n",
            "Train Epoch: 15 [51200/55000 (93%)]\tLoss: 0.013437\n",
            "\n",
            "Validation set: Avg. loss: 0.000228\n",
            "\n",
            "Train Epoch: 16 [0/55000 (0%)]\tLoss: 0.014023\n",
            "Train Epoch: 16 [12800/55000 (23%)]\tLoss: 0.014648\n",
            "Train Epoch: 16 [25600/55000 (47%)]\tLoss: 0.013126\n",
            "Train Epoch: 16 [38400/55000 (70%)]\tLoss: 0.013433\n",
            "Train Epoch: 16 [51200/55000 (93%)]\tLoss: 0.012855\n",
            "\n",
            "Validation set: Avg. loss: 0.000227\n",
            "\n",
            "Train Epoch: 17 [0/55000 (0%)]\tLoss: 0.015252\n",
            "Train Epoch: 17 [12800/55000 (23%)]\tLoss: 0.014123\n",
            "Train Epoch: 17 [25600/55000 (47%)]\tLoss: 0.014150\n",
            "Train Epoch: 17 [38400/55000 (70%)]\tLoss: 0.014993\n",
            "Train Epoch: 17 [51200/55000 (93%)]\tLoss: 0.011900\n",
            "\n",
            "Validation set: Avg. loss: 0.000226\n",
            "\n",
            "Train Epoch: 18 [0/55000 (0%)]\tLoss: 0.012782\n",
            "Train Epoch: 18 [12800/55000 (23%)]\tLoss: 0.011384\n",
            "Train Epoch: 18 [25600/55000 (47%)]\tLoss: 0.012019\n",
            "Train Epoch: 18 [38400/55000 (70%)]\tLoss: 0.012516\n",
            "Train Epoch: 18 [51200/55000 (93%)]\tLoss: 0.014513\n",
            "\n",
            "Validation set: Avg. loss: 0.000225\n",
            "\n",
            "Train Epoch: 19 [0/55000 (0%)]\tLoss: 0.014140\n",
            "Train Epoch: 19 [12800/55000 (23%)]\tLoss: 0.014333\n",
            "Train Epoch: 19 [25600/55000 (47%)]\tLoss: 0.011219\n",
            "Train Epoch: 19 [38400/55000 (70%)]\tLoss: 0.013127\n",
            "Train Epoch: 19 [51200/55000 (93%)]\tLoss: 0.013809\n",
            "\n",
            "Validation set: Avg. loss: 0.000223\n",
            "\n",
            "Train Epoch: 20 [0/55000 (0%)]\tLoss: 0.012914\n",
            "Train Epoch: 20 [12800/55000 (23%)]\tLoss: 0.014717\n",
            "Train Epoch: 20 [25600/55000 (47%)]\tLoss: 0.012106\n",
            "Train Epoch: 20 [38400/55000 (70%)]\tLoss: 0.014788\n",
            "Train Epoch: 20 [51200/55000 (93%)]\tLoss: 0.014617\n",
            "\n",
            "Validation set: Avg. loss: 0.000221\n",
            "\n",
            "Train Epoch: 21 [0/55000 (0%)]\tLoss: 0.014193\n",
            "Train Epoch: 21 [12800/55000 (23%)]\tLoss: 0.012553\n",
            "Train Epoch: 21 [25600/55000 (47%)]\tLoss: 0.013379\n",
            "Train Epoch: 21 [38400/55000 (70%)]\tLoss: 0.013071\n",
            "Train Epoch: 21 [51200/55000 (93%)]\tLoss: 0.013508\n",
            "\n",
            "Validation set: Avg. loss: 0.000220\n",
            "\n",
            "Train Epoch: 22 [0/55000 (0%)]\tLoss: 0.013743\n",
            "Train Epoch: 22 [12800/55000 (23%)]\tLoss: 0.013938\n",
            "Train Epoch: 22 [25600/55000 (47%)]\tLoss: 0.013621\n",
            "Train Epoch: 22 [38400/55000 (70%)]\tLoss: 0.014472\n",
            "Train Epoch: 22 [51200/55000 (93%)]\tLoss: 0.012856\n",
            "\n",
            "Validation set: Avg. loss: 0.000222\n",
            "\n",
            "Train Epoch: 23 [0/55000 (0%)]\tLoss: 0.013909\n",
            "Train Epoch: 23 [12800/55000 (23%)]\tLoss: 0.010611\n",
            "Train Epoch: 23 [25600/55000 (47%)]\tLoss: 0.013095\n",
            "Train Epoch: 23 [38400/55000 (70%)]\tLoss: 0.013065\n",
            "Train Epoch: 23 [51200/55000 (93%)]\tLoss: 0.015374\n",
            "\n",
            "Validation set: Avg. loss: 0.000220\n",
            "\n",
            "Train Epoch: 24 [0/55000 (0%)]\tLoss: 0.014775\n",
            "Train Epoch: 24 [12800/55000 (23%)]\tLoss: 0.014246\n",
            "Train Epoch: 24 [25600/55000 (47%)]\tLoss: 0.012419\n",
            "Train Epoch: 24 [38400/55000 (70%)]\tLoss: 0.013005\n",
            "Train Epoch: 24 [51200/55000 (93%)]\tLoss: 0.013384\n",
            "\n",
            "Validation set: Avg. loss: 0.000218\n",
            "\n",
            "Train Epoch: 25 [0/55000 (0%)]\tLoss: 0.013276\n",
            "Train Epoch: 25 [12800/55000 (23%)]\tLoss: 0.015771\n",
            "Train Epoch: 25 [25600/55000 (47%)]\tLoss: 0.013013\n",
            "Train Epoch: 25 [38400/55000 (70%)]\tLoss: 0.014693\n",
            "Train Epoch: 25 [51200/55000 (93%)]\tLoss: 0.013976\n",
            "\n",
            "Validation set: Avg. loss: 0.000218\n",
            "\n",
            "Train Epoch: 26 [0/55000 (0%)]\tLoss: 0.014808\n",
            "Train Epoch: 26 [12800/55000 (23%)]\tLoss: 0.012204\n",
            "Train Epoch: 26 [25600/55000 (47%)]\tLoss: 0.012830\n",
            "Train Epoch: 26 [38400/55000 (70%)]\tLoss: 0.013819\n",
            "Train Epoch: 26 [51200/55000 (93%)]\tLoss: 0.015382\n",
            "\n",
            "Validation set: Avg. loss: 0.000217\n",
            "\n",
            "Train Epoch: 27 [0/55000 (0%)]\tLoss: 0.013450\n",
            "Train Epoch: 27 [12800/55000 (23%)]\tLoss: 0.014312\n",
            "Train Epoch: 27 [25600/55000 (47%)]\tLoss: 0.013600\n",
            "Train Epoch: 27 [38400/55000 (70%)]\tLoss: 0.011866\n",
            "Train Epoch: 27 [51200/55000 (93%)]\tLoss: 0.014261\n",
            "\n",
            "Validation set: Avg. loss: 0.000215\n",
            "\n",
            "Train Epoch: 28 [0/55000 (0%)]\tLoss: 0.011707\n",
            "Train Epoch: 28 [12800/55000 (23%)]\tLoss: 0.012680\n",
            "Train Epoch: 28 [25600/55000 (47%)]\tLoss: 0.012964\n",
            "Train Epoch: 28 [38400/55000 (70%)]\tLoss: 0.015365\n",
            "Train Epoch: 28 [51200/55000 (93%)]\tLoss: 0.014913\n",
            "\n",
            "Validation set: Avg. loss: 0.000215\n",
            "\n",
            "Train Epoch: 29 [0/55000 (0%)]\tLoss: 0.013727\n",
            "Train Epoch: 29 [12800/55000 (23%)]\tLoss: 0.012744\n",
            "Train Epoch: 29 [25600/55000 (47%)]\tLoss: 0.013629\n",
            "Train Epoch: 29 [38400/55000 (70%)]\tLoss: 0.014547\n",
            "Train Epoch: 29 [51200/55000 (93%)]\tLoss: 0.010564\n",
            "\n",
            "Validation set: Avg. loss: 0.000214\n",
            "\n",
            "Train Epoch: 30 [0/55000 (0%)]\tLoss: 0.011719\n",
            "Train Epoch: 30 [12800/55000 (23%)]\tLoss: 0.011532\n",
            "Train Epoch: 30 [25600/55000 (47%)]\tLoss: 0.011773\n",
            "Train Epoch: 30 [38400/55000 (70%)]\tLoss: 0.014089\n",
            "Train Epoch: 30 [51200/55000 (93%)]\tLoss: 0.013587\n",
            "\n",
            "Validation set: Avg. loss: 0.000215\n",
            "\n",
            "Train Epoch: 31 [0/55000 (0%)]\tLoss: 0.013374\n",
            "Train Epoch: 31 [12800/55000 (23%)]\tLoss: 0.013433\n",
            "Train Epoch: 31 [25600/55000 (47%)]\tLoss: 0.012967\n",
            "Train Epoch: 31 [38400/55000 (70%)]\tLoss: 0.013437\n",
            "Train Epoch: 31 [51200/55000 (93%)]\tLoss: 0.012481\n",
            "\n",
            "Validation set: Avg. loss: 0.000214\n",
            "\n",
            "Train Epoch: 32 [0/55000 (0%)]\tLoss: 0.011853\n",
            "Train Epoch: 32 [12800/55000 (23%)]\tLoss: 0.013932\n",
            "Train Epoch: 32 [25600/55000 (47%)]\tLoss: 0.014087\n",
            "Train Epoch: 32 [38400/55000 (70%)]\tLoss: 0.012191\n",
            "Train Epoch: 32 [51200/55000 (93%)]\tLoss: 0.013312\n",
            "\n",
            "Validation set: Avg. loss: 0.000212\n",
            "\n",
            "Train Epoch: 33 [0/55000 (0%)]\tLoss: 0.012842\n",
            "Train Epoch: 33 [12800/55000 (23%)]\tLoss: 0.012503\n",
            "Train Epoch: 33 [25600/55000 (47%)]\tLoss: 0.014007\n",
            "Train Epoch: 33 [38400/55000 (70%)]\tLoss: 0.012851\n",
            "Train Epoch: 33 [51200/55000 (93%)]\tLoss: 0.013585\n",
            "\n",
            "Validation set: Avg. loss: 0.000213\n",
            "\n",
            "Train Epoch: 34 [0/55000 (0%)]\tLoss: 0.011947\n",
            "Train Epoch: 34 [12800/55000 (23%)]\tLoss: 0.014594\n",
            "Train Epoch: 34 [25600/55000 (47%)]\tLoss: 0.012953\n",
            "Train Epoch: 34 [38400/55000 (70%)]\tLoss: 0.012273\n",
            "Train Epoch: 34 [51200/55000 (93%)]\tLoss: 0.013448\n",
            "\n",
            "Validation set: Avg. loss: 0.000213\n",
            "\n",
            "Train Epoch: 35 [0/55000 (0%)]\tLoss: 0.014428\n",
            "Train Epoch: 35 [12800/55000 (23%)]\tLoss: 0.013645\n",
            "Train Epoch: 35 [25600/55000 (47%)]\tLoss: 0.013379\n",
            "Train Epoch: 35 [38400/55000 (70%)]\tLoss: 0.012907\n",
            "Train Epoch: 35 [51200/55000 (93%)]\tLoss: 0.013334\n",
            "\n",
            "Validation set: Avg. loss: 0.000212\n",
            "\n",
            "Train Epoch: 36 [0/55000 (0%)]\tLoss: 0.011793\n",
            "Train Epoch: 36 [12800/55000 (23%)]\tLoss: 0.012157\n",
            "Train Epoch: 36 [25600/55000 (47%)]\tLoss: 0.011795\n",
            "Train Epoch: 36 [38400/55000 (70%)]\tLoss: 0.015111\n",
            "Train Epoch: 36 [51200/55000 (93%)]\tLoss: 0.015308\n",
            "\n",
            "Validation set: Avg. loss: 0.000213\n",
            "\n",
            "Train Epoch: 37 [0/55000 (0%)]\tLoss: 0.014497\n",
            "Train Epoch: 37 [12800/55000 (23%)]\tLoss: 0.011392\n",
            "Train Epoch: 37 [25600/55000 (47%)]\tLoss: 0.012471\n",
            "Train Epoch: 37 [38400/55000 (70%)]\tLoss: 0.013831\n",
            "Train Epoch: 37 [51200/55000 (93%)]\tLoss: 0.013136\n",
            "\n",
            "Validation set: Avg. loss: 0.000211\n",
            "\n",
            "Train Epoch: 38 [0/55000 (0%)]\tLoss: 0.012651\n",
            "Train Epoch: 38 [12800/55000 (23%)]\tLoss: 0.012913\n",
            "Train Epoch: 38 [25600/55000 (47%)]\tLoss: 0.014927\n",
            "Train Epoch: 38 [38400/55000 (70%)]\tLoss: 0.011961\n",
            "Train Epoch: 38 [51200/55000 (93%)]\tLoss: 0.012469\n",
            "\n",
            "Validation set: Avg. loss: 0.000209\n",
            "\n",
            "Train Epoch: 39 [0/55000 (0%)]\tLoss: 0.012864\n",
            "Train Epoch: 39 [12800/55000 (23%)]\tLoss: 0.014161\n",
            "Train Epoch: 39 [25600/55000 (47%)]\tLoss: 0.013348\n",
            "Train Epoch: 39 [38400/55000 (70%)]\tLoss: 0.014171\n",
            "Train Epoch: 39 [51200/55000 (93%)]\tLoss: 0.012566\n",
            "\n",
            "Validation set: Avg. loss: 0.000210\n",
            "\n",
            "Train Epoch: 40 [0/55000 (0%)]\tLoss: 0.011119\n",
            "Train Epoch: 40 [12800/55000 (23%)]\tLoss: 0.012870\n",
            "Train Epoch: 40 [25600/55000 (47%)]\tLoss: 0.012497\n",
            "Train Epoch: 40 [38400/55000 (70%)]\tLoss: 0.012269\n",
            "Train Epoch: 40 [51200/55000 (93%)]\tLoss: 0.013517\n",
            "\n",
            "Validation set: Avg. loss: 0.000210\n",
            "\n",
            "Train Epoch: 41 [0/55000 (0%)]\tLoss: 0.011648\n",
            "Train Epoch: 41 [12800/55000 (23%)]\tLoss: 0.014317\n",
            "Train Epoch: 41 [25600/55000 (47%)]\tLoss: 0.013448\n",
            "Train Epoch: 41 [38400/55000 (70%)]\tLoss: 0.011825\n",
            "Train Epoch: 41 [51200/55000 (93%)]\tLoss: 0.011724\n",
            "\n",
            "Validation set: Avg. loss: 0.000211\n",
            "\n",
            "Train Epoch: 42 [0/55000 (0%)]\tLoss: 0.013274\n",
            "Train Epoch: 42 [12800/55000 (23%)]\tLoss: 0.012591\n",
            "Train Epoch: 42 [25600/55000 (47%)]\tLoss: 0.013382\n",
            "Train Epoch: 42 [38400/55000 (70%)]\tLoss: 0.012074\n",
            "Train Epoch: 42 [51200/55000 (93%)]\tLoss: 0.011511\n",
            "\n",
            "Validation set: Avg. loss: 0.000209\n",
            "\n",
            "Train Epoch: 43 [0/55000 (0%)]\tLoss: 0.013027\n",
            "Train Epoch: 43 [12800/55000 (23%)]\tLoss: 0.012106\n",
            "Train Epoch: 43 [25600/55000 (47%)]\tLoss: 0.014034\n",
            "Train Epoch: 43 [38400/55000 (70%)]\tLoss: 0.012427\n",
            "Train Epoch: 43 [51200/55000 (93%)]\tLoss: 0.014339\n",
            "\n",
            "Validation set: Avg. loss: 0.000210\n",
            "\n",
            "Train Epoch: 44 [0/55000 (0%)]\tLoss: 0.011023\n",
            "Train Epoch: 44 [12800/55000 (23%)]\tLoss: 0.011039\n",
            "Train Epoch: 44 [25600/55000 (47%)]\tLoss: 0.012573\n",
            "Train Epoch: 44 [38400/55000 (70%)]\tLoss: 0.011382\n",
            "Train Epoch: 44 [51200/55000 (93%)]\tLoss: 0.012470\n",
            "\n",
            "Validation set: Avg. loss: 0.000208\n",
            "\n",
            "Train Epoch: 45 [0/55000 (0%)]\tLoss: 0.010902\n",
            "Train Epoch: 45 [12800/55000 (23%)]\tLoss: 0.011341\n",
            "Train Epoch: 45 [25600/55000 (47%)]\tLoss: 0.013157\n",
            "Train Epoch: 45 [38400/55000 (70%)]\tLoss: 0.011528\n",
            "Train Epoch: 45 [51200/55000 (93%)]\tLoss: 0.011672\n",
            "\n",
            "Validation set: Avg. loss: 0.000208\n",
            "\n",
            "Train Epoch: 46 [0/55000 (0%)]\tLoss: 0.012256\n",
            "Train Epoch: 46 [12800/55000 (23%)]\tLoss: 0.014166\n",
            "Train Epoch: 46 [25600/55000 (47%)]\tLoss: 0.013027\n",
            "Train Epoch: 46 [38400/55000 (70%)]\tLoss: 0.013401\n",
            "Train Epoch: 46 [51200/55000 (93%)]\tLoss: 0.012868\n",
            "\n",
            "Validation set: Avg. loss: 0.000209\n",
            "\n",
            "Train Epoch: 47 [0/55000 (0%)]\tLoss: 0.012101\n",
            "Train Epoch: 47 [12800/55000 (23%)]\tLoss: 0.012284\n",
            "Train Epoch: 47 [25600/55000 (47%)]\tLoss: 0.014420\n",
            "Train Epoch: 47 [38400/55000 (70%)]\tLoss: 0.014214\n",
            "Train Epoch: 47 [51200/55000 (93%)]\tLoss: 0.014136\n",
            "\n",
            "Validation set: Avg. loss: 0.000208\n",
            "\n",
            "Train Epoch: 48 [0/55000 (0%)]\tLoss: 0.013516\n",
            "Train Epoch: 48 [12800/55000 (23%)]\tLoss: 0.013523\n",
            "Train Epoch: 48 [25600/55000 (47%)]\tLoss: 0.014072\n",
            "Train Epoch: 48 [38400/55000 (70%)]\tLoss: 0.011164\n",
            "Train Epoch: 48 [51200/55000 (93%)]\tLoss: 0.012360\n",
            "\n",
            "Validation set: Avg. loss: 0.000207\n",
            "\n",
            "Train Epoch: 49 [0/55000 (0%)]\tLoss: 0.011575\n",
            "Train Epoch: 49 [12800/55000 (23%)]\tLoss: 0.012938\n",
            "Train Epoch: 49 [25600/55000 (47%)]\tLoss: 0.012000\n",
            "Train Epoch: 49 [38400/55000 (70%)]\tLoss: 0.010631\n",
            "Train Epoch: 49 [51200/55000 (93%)]\tLoss: 0.011362\n",
            "\n",
            "Validation set: Avg. loss: 0.000207\n",
            "\n",
            "Train Epoch: 50 [0/55000 (0%)]\tLoss: 0.011522\n",
            "Train Epoch: 50 [12800/55000 (23%)]\tLoss: 0.014466\n",
            "Train Epoch: 50 [25600/55000 (47%)]\tLoss: 0.012459\n",
            "Train Epoch: 50 [38400/55000 (70%)]\tLoss: 0.014479\n",
            "Train Epoch: 50 [51200/55000 (93%)]\tLoss: 0.015136\n",
            "\n",
            "Validation set: Avg. loss: 0.000207\n",
            "\n",
            "Train Epoch: 51 [0/55000 (0%)]\tLoss: 0.011750\n",
            "Train Epoch: 51 [12800/55000 (23%)]\tLoss: 0.012973\n",
            "Train Epoch: 51 [25600/55000 (47%)]\tLoss: 0.013467\n",
            "Train Epoch: 51 [38400/55000 (70%)]\tLoss: 0.013056\n",
            "Train Epoch: 51 [51200/55000 (93%)]\tLoss: 0.013222\n",
            "\n",
            "Validation set: Avg. loss: 0.000207\n",
            "\n",
            "Train Epoch: 52 [0/55000 (0%)]\tLoss: 0.011880\n",
            "Train Epoch: 52 [12800/55000 (23%)]\tLoss: 0.012712\n",
            "Train Epoch: 52 [25600/55000 (47%)]\tLoss: 0.012196\n",
            "Train Epoch: 52 [38400/55000 (70%)]\tLoss: 0.013032\n",
            "Train Epoch: 52 [51200/55000 (93%)]\tLoss: 0.012070\n",
            "\n",
            "Validation set: Avg. loss: 0.000206\n",
            "\n",
            "Train Epoch: 53 [0/55000 (0%)]\tLoss: 0.013068\n",
            "Train Epoch: 53 [12800/55000 (23%)]\tLoss: 0.012372\n",
            "Train Epoch: 53 [25600/55000 (47%)]\tLoss: 0.012250\n",
            "Train Epoch: 53 [38400/55000 (70%)]\tLoss: 0.012198\n",
            "Train Epoch: 53 [51200/55000 (93%)]\tLoss: 0.013563\n",
            "\n",
            "Validation set: Avg. loss: 0.000206\n",
            "\n",
            "Train Epoch: 54 [0/55000 (0%)]\tLoss: 0.011305\n",
            "Train Epoch: 54 [12800/55000 (23%)]\tLoss: 0.012611\n",
            "Train Epoch: 54 [25600/55000 (47%)]\tLoss: 0.011089\n",
            "Train Epoch: 54 [38400/55000 (70%)]\tLoss: 0.012754\n",
            "Train Epoch: 54 [51200/55000 (93%)]\tLoss: 0.014546\n",
            "\n",
            "Validation set: Avg. loss: 0.000206\n",
            "\n",
            "Train Epoch: 55 [0/55000 (0%)]\tLoss: 0.011892\n",
            "Train Epoch: 55 [12800/55000 (23%)]\tLoss: 0.015091\n",
            "Train Epoch: 55 [25600/55000 (47%)]\tLoss: 0.012290\n",
            "Train Epoch: 55 [38400/55000 (70%)]\tLoss: 0.010256\n",
            "Train Epoch: 55 [51200/55000 (93%)]\tLoss: 0.012255\n",
            "\n",
            "Validation set: Avg. loss: 0.000206\n",
            "\n",
            "Train Epoch: 56 [0/55000 (0%)]\tLoss: 0.012567\n",
            "Train Epoch: 56 [12800/55000 (23%)]\tLoss: 0.011420\n",
            "Train Epoch: 56 [25600/55000 (47%)]\tLoss: 0.011512\n",
            "Train Epoch: 56 [38400/55000 (70%)]\tLoss: 0.012430\n",
            "Train Epoch: 56 [51200/55000 (93%)]\tLoss: 0.012673\n",
            "\n",
            "Validation set: Avg. loss: 0.000206\n",
            "\n",
            "Train Epoch: 57 [0/55000 (0%)]\tLoss: 0.012150\n",
            "Train Epoch: 57 [12800/55000 (23%)]\tLoss: 0.011743\n",
            "Train Epoch: 57 [25600/55000 (47%)]\tLoss: 0.013166\n",
            "Train Epoch: 57 [38400/55000 (70%)]\tLoss: 0.014161\n",
            "Train Epoch: 57 [51200/55000 (93%)]\tLoss: 0.012834\n",
            "\n",
            "Validation set: Avg. loss: 0.000207\n",
            "\n",
            "Train Epoch: 58 [0/55000 (0%)]\tLoss: 0.011120\n",
            "Train Epoch: 58 [12800/55000 (23%)]\tLoss: 0.011911\n",
            "Train Epoch: 58 [25600/55000 (47%)]\tLoss: 0.011859\n",
            "Train Epoch: 58 [38400/55000 (70%)]\tLoss: 0.012877\n",
            "Train Epoch: 58 [51200/55000 (93%)]\tLoss: 0.014282\n",
            "\n",
            "Validation set: Avg. loss: 0.000206\n",
            "\n",
            "Train Epoch: 59 [0/55000 (0%)]\tLoss: 0.011602\n",
            "Train Epoch: 59 [12800/55000 (23%)]\tLoss: 0.012522\n",
            "Train Epoch: 59 [25600/55000 (47%)]\tLoss: 0.013443\n",
            "Train Epoch: 59 [38400/55000 (70%)]\tLoss: 0.011675\n",
            "Train Epoch: 59 [51200/55000 (93%)]\tLoss: 0.011884\n",
            "\n",
            "Validation set: Avg. loss: 0.000206\n",
            "\n",
            "Train Epoch: 60 [0/55000 (0%)]\tLoss: 0.011206\n",
            "Train Epoch: 60 [12800/55000 (23%)]\tLoss: 0.011245\n",
            "Train Epoch: 60 [25600/55000 (47%)]\tLoss: 0.012136\n",
            "Train Epoch: 60 [38400/55000 (70%)]\tLoss: 0.011114\n",
            "Train Epoch: 60 [51200/55000 (93%)]\tLoss: 0.011797\n",
            "\n",
            "Validation set: Avg. loss: 0.000206\n",
            "\n",
            "Train Epoch: 61 [0/55000 (0%)]\tLoss: 0.014539\n",
            "Train Epoch: 61 [12800/55000 (23%)]\tLoss: 0.012210\n",
            "Train Epoch: 61 [25600/55000 (47%)]\tLoss: 0.012671\n",
            "Train Epoch: 61 [38400/55000 (70%)]\tLoss: 0.011593\n",
            "Train Epoch: 61 [51200/55000 (93%)]\tLoss: 0.012911\n",
            "\n",
            "Validation set: Avg. loss: 0.000204\n",
            "\n",
            "Train Epoch: 62 [0/55000 (0%)]\tLoss: 0.009984\n",
            "Train Epoch: 62 [12800/55000 (23%)]\tLoss: 0.012896\n",
            "Train Epoch: 62 [25600/55000 (47%)]\tLoss: 0.010626\n",
            "Train Epoch: 62 [38400/55000 (70%)]\tLoss: 0.012754\n",
            "Train Epoch: 62 [51200/55000 (93%)]\tLoss: 0.012237\n",
            "\n",
            "Validation set: Avg. loss: 0.000205\n",
            "\n",
            "Train Epoch: 63 [0/55000 (0%)]\tLoss: 0.012285\n",
            "Train Epoch: 63 [12800/55000 (23%)]\tLoss: 0.012144\n",
            "Train Epoch: 63 [25600/55000 (47%)]\tLoss: 0.011717\n",
            "Train Epoch: 63 [38400/55000 (70%)]\tLoss: 0.012472\n",
            "Train Epoch: 63 [51200/55000 (93%)]\tLoss: 0.011616\n",
            "\n",
            "Validation set: Avg. loss: 0.000206\n",
            "\n",
            "Train Epoch: 64 [0/55000 (0%)]\tLoss: 0.012616\n",
            "Train Epoch: 64 [12800/55000 (23%)]\tLoss: 0.012458\n",
            "Train Epoch: 64 [25600/55000 (47%)]\tLoss: 0.011925\n",
            "Train Epoch: 64 [38400/55000 (70%)]\tLoss: 0.013552\n",
            "Train Epoch: 64 [51200/55000 (93%)]\tLoss: 0.010717\n",
            "\n",
            "Validation set: Avg. loss: 0.000205\n",
            "\n",
            "Train Epoch: 65 [0/55000 (0%)]\tLoss: 0.013590\n",
            "Train Epoch: 65 [12800/55000 (23%)]\tLoss: 0.012465\n",
            "Train Epoch: 65 [25600/55000 (47%)]\tLoss: 0.013473\n",
            "Train Epoch: 65 [38400/55000 (70%)]\tLoss: 0.011907\n",
            "Train Epoch: 65 [51200/55000 (93%)]\tLoss: 0.012843\n",
            "\n",
            "Validation set: Avg. loss: 0.000205\n",
            "\n",
            "Train Epoch: 66 [0/55000 (0%)]\tLoss: 0.011597\n",
            "Train Epoch: 66 [12800/55000 (23%)]\tLoss: 0.011570\n",
            "Train Epoch: 66 [25600/55000 (47%)]\tLoss: 0.011729\n",
            "Train Epoch: 66 [38400/55000 (70%)]\tLoss: 0.013043\n",
            "Train Epoch: 66 [51200/55000 (93%)]\tLoss: 0.012905\n",
            "\n",
            "Validation set: Avg. loss: 0.000204\n",
            "\n",
            "Train Epoch: 67 [0/55000 (0%)]\tLoss: 0.012356\n",
            "Train Epoch: 67 [12800/55000 (23%)]\tLoss: 0.012259\n",
            "Train Epoch: 67 [25600/55000 (47%)]\tLoss: 0.011767\n",
            "Train Epoch: 67 [38400/55000 (70%)]\tLoss: 0.011321\n",
            "Train Epoch: 67 [51200/55000 (93%)]\tLoss: 0.012842\n",
            "\n",
            "Validation set: Avg. loss: 0.000207\n",
            "\n",
            "Train Epoch: 68 [0/55000 (0%)]\tLoss: 0.010568\n",
            "Train Epoch: 68 [12800/55000 (23%)]\tLoss: 0.012928\n",
            "Train Epoch: 68 [25600/55000 (47%)]\tLoss: 0.010791\n",
            "Train Epoch: 68 [38400/55000 (70%)]\tLoss: 0.013386\n",
            "Train Epoch: 68 [51200/55000 (93%)]\tLoss: 0.013141\n",
            "\n",
            "Validation set: Avg. loss: 0.000203\n",
            "\n",
            "Train Epoch: 69 [0/55000 (0%)]\tLoss: 0.011172\n",
            "Train Epoch: 69 [12800/55000 (23%)]\tLoss: 0.010905\n",
            "Train Epoch: 69 [25600/55000 (47%)]\tLoss: 0.011734\n",
            "Train Epoch: 69 [38400/55000 (70%)]\tLoss: 0.011709\n",
            "Train Epoch: 69 [51200/55000 (93%)]\tLoss: 0.014967\n",
            "\n",
            "Validation set: Avg. loss: 0.000203\n",
            "\n",
            "Train Epoch: 70 [0/55000 (0%)]\tLoss: 0.011073\n",
            "Train Epoch: 70 [12800/55000 (23%)]\tLoss: 0.012867\n",
            "Train Epoch: 70 [25600/55000 (47%)]\tLoss: 0.010973\n",
            "Train Epoch: 70 [38400/55000 (70%)]\tLoss: 0.012442\n",
            "Train Epoch: 70 [51200/55000 (93%)]\tLoss: 0.011577\n",
            "\n",
            "Validation set: Avg. loss: 0.000204\n",
            "\n",
            "Train Epoch: 71 [0/55000 (0%)]\tLoss: 0.011711\n",
            "Train Epoch: 71 [12800/55000 (23%)]\tLoss: 0.011388\n",
            "Train Epoch: 71 [25600/55000 (47%)]\tLoss: 0.012730\n",
            "Train Epoch: 71 [38400/55000 (70%)]\tLoss: 0.010873\n",
            "Train Epoch: 71 [51200/55000 (93%)]\tLoss: 0.013825\n",
            "\n",
            "Validation set: Avg. loss: 0.000203\n",
            "\n",
            "Train Epoch: 72 [0/55000 (0%)]\tLoss: 0.012728\n",
            "Train Epoch: 72 [12800/55000 (23%)]\tLoss: 0.012240\n",
            "Train Epoch: 72 [25600/55000 (47%)]\tLoss: 0.013053\n",
            "Train Epoch: 72 [38400/55000 (70%)]\tLoss: 0.012249\n",
            "Train Epoch: 72 [51200/55000 (93%)]\tLoss: 0.011751\n",
            "\n",
            "Validation set: Avg. loss: 0.000204\n",
            "\n",
            "Train Epoch: 73 [0/55000 (0%)]\tLoss: 0.011017\n",
            "Train Epoch: 73 [12800/55000 (23%)]\tLoss: 0.012635\n",
            "Train Epoch: 73 [25600/55000 (47%)]\tLoss: 0.012458\n",
            "Train Epoch: 73 [38400/55000 (70%)]\tLoss: 0.011372\n",
            "Train Epoch: 73 [51200/55000 (93%)]\tLoss: 0.013358\n",
            "\n",
            "Validation set: Avg. loss: 0.000203\n",
            "\n",
            "Train Epoch: 74 [0/55000 (0%)]\tLoss: 0.010331\n",
            "Train Epoch: 74 [12800/55000 (23%)]\tLoss: 0.011531\n",
            "Train Epoch: 74 [25600/55000 (47%)]\tLoss: 0.011909\n",
            "Train Epoch: 74 [38400/55000 (70%)]\tLoss: 0.012882\n",
            "Train Epoch: 74 [51200/55000 (93%)]\tLoss: 0.011256\n",
            "\n",
            "Validation set: Avg. loss: 0.000203\n",
            "\n",
            "Train Epoch: 75 [0/55000 (0%)]\tLoss: 0.010458\n",
            "Train Epoch: 75 [12800/55000 (23%)]\tLoss: 0.010319\n",
            "Train Epoch: 75 [25600/55000 (47%)]\tLoss: 0.012422\n",
            "Train Epoch: 75 [38400/55000 (70%)]\tLoss: 0.011019\n",
            "Train Epoch: 75 [51200/55000 (93%)]\tLoss: 0.011870\n",
            "\n",
            "Validation set: Avg. loss: 0.000203\n",
            "\n",
            "Train Epoch: 76 [0/55000 (0%)]\tLoss: 0.012326\n",
            "Train Epoch: 76 [12800/55000 (23%)]\tLoss: 0.011635\n",
            "Train Epoch: 76 [25600/55000 (47%)]\tLoss: 0.012844\n",
            "Train Epoch: 76 [38400/55000 (70%)]\tLoss: 0.012624\n",
            "Train Epoch: 76 [51200/55000 (93%)]\tLoss: 0.010667\n",
            "\n",
            "Validation set: Avg. loss: 0.000203\n",
            "\n",
            "Train Epoch: 77 [0/55000 (0%)]\tLoss: 0.011713\n",
            "Train Epoch: 77 [12800/55000 (23%)]\tLoss: 0.011404\n",
            "Train Epoch: 77 [25600/55000 (47%)]\tLoss: 0.012288\n",
            "Train Epoch: 77 [38400/55000 (70%)]\tLoss: 0.011462\n",
            "Train Epoch: 77 [51200/55000 (93%)]\tLoss: 0.014362\n",
            "\n",
            "Validation set: Avg. loss: 0.000202\n",
            "\n",
            "Train Epoch: 78 [0/55000 (0%)]\tLoss: 0.013317\n",
            "Train Epoch: 78 [12800/55000 (23%)]\tLoss: 0.012780\n",
            "Train Epoch: 78 [25600/55000 (47%)]\tLoss: 0.011400\n",
            "Train Epoch: 78 [38400/55000 (70%)]\tLoss: 0.012508\n",
            "Train Epoch: 78 [51200/55000 (93%)]\tLoss: 0.012206\n",
            "\n",
            "Validation set: Avg. loss: 0.000201\n",
            "\n",
            "Train Epoch: 79 [0/55000 (0%)]\tLoss: 0.010796\n",
            "Train Epoch: 79 [12800/55000 (23%)]\tLoss: 0.010942\n",
            "Train Epoch: 79 [25600/55000 (47%)]\tLoss: 0.011908\n",
            "Train Epoch: 79 [38400/55000 (70%)]\tLoss: 0.012299\n",
            "Train Epoch: 79 [51200/55000 (93%)]\tLoss: 0.012801\n",
            "\n",
            "Validation set: Avg. loss: 0.000204\n",
            "\n",
            "Train Epoch: 80 [0/55000 (0%)]\tLoss: 0.010360\n",
            "Train Epoch: 80 [12800/55000 (23%)]\tLoss: 0.013135\n",
            "Train Epoch: 80 [25600/55000 (47%)]\tLoss: 0.010355\n",
            "Train Epoch: 80 [38400/55000 (70%)]\tLoss: 0.012397\n",
            "Train Epoch: 80 [51200/55000 (93%)]\tLoss: 0.012999\n",
            "\n",
            "Validation set: Avg. loss: 0.000203\n",
            "\n",
            "Train Epoch: 81 [0/55000 (0%)]\tLoss: 0.013358\n",
            "Train Epoch: 81 [12800/55000 (23%)]\tLoss: 0.011837\n",
            "Train Epoch: 81 [25600/55000 (47%)]\tLoss: 0.012077\n",
            "Train Epoch: 81 [38400/55000 (70%)]\tLoss: 0.011451\n",
            "Train Epoch: 81 [51200/55000 (93%)]\tLoss: 0.011686\n",
            "\n",
            "Validation set: Avg. loss: 0.000203\n",
            "\n",
            "Train Epoch: 82 [0/55000 (0%)]\tLoss: 0.012692\n",
            "Train Epoch: 82 [12800/55000 (23%)]\tLoss: 0.011609\n",
            "Train Epoch: 82 [25600/55000 (47%)]\tLoss: 0.012045\n",
            "Train Epoch: 82 [38400/55000 (70%)]\tLoss: 0.014148\n",
            "Train Epoch: 82 [51200/55000 (93%)]\tLoss: 0.012716\n",
            "\n",
            "Validation set: Avg. loss: 0.000202\n",
            "\n",
            "Train Epoch: 83 [0/55000 (0%)]\tLoss: 0.011506\n",
            "Train Epoch: 83 [12800/55000 (23%)]\tLoss: 0.013139\n",
            "Train Epoch: 83 [25600/55000 (47%)]\tLoss: 0.011438\n",
            "Train Epoch: 83 [38400/55000 (70%)]\tLoss: 0.009379\n",
            "Train Epoch: 83 [51200/55000 (93%)]\tLoss: 0.013130\n",
            "\n",
            "Validation set: Avg. loss: 0.000202\n",
            "\n",
            "Train Epoch: 84 [0/55000 (0%)]\tLoss: 0.010658\n",
            "Train Epoch: 84 [12800/55000 (23%)]\tLoss: 0.012397\n",
            "Train Epoch: 84 [25600/55000 (47%)]\tLoss: 0.010428\n",
            "Train Epoch: 84 [38400/55000 (70%)]\tLoss: 0.012737\n",
            "Train Epoch: 84 [51200/55000 (93%)]\tLoss: 0.011517\n",
            "\n",
            "Validation set: Avg. loss: 0.000202\n",
            "\n",
            "Train Epoch: 85 [0/55000 (0%)]\tLoss: 0.011415\n",
            "Train Epoch: 85 [12800/55000 (23%)]\tLoss: 0.012113\n",
            "Train Epoch: 85 [25600/55000 (47%)]\tLoss: 0.011338\n",
            "Train Epoch: 85 [38400/55000 (70%)]\tLoss: 0.010784\n",
            "Train Epoch: 85 [51200/55000 (93%)]\tLoss: 0.011684\n",
            "\n",
            "Validation set: Avg. loss: 0.000202\n",
            "\n",
            "Train Epoch: 86 [0/55000 (0%)]\tLoss: 0.011409\n",
            "Train Epoch: 86 [12800/55000 (23%)]\tLoss: 0.010979\n",
            "Train Epoch: 86 [25600/55000 (47%)]\tLoss: 0.012382\n",
            "Train Epoch: 86 [38400/55000 (70%)]\tLoss: 0.011847\n",
            "Train Epoch: 86 [51200/55000 (93%)]\tLoss: 0.010166\n",
            "\n",
            "Validation set: Avg. loss: 0.000205\n",
            "\n",
            "Train Epoch: 87 [0/55000 (0%)]\tLoss: 0.011418\n",
            "Train Epoch: 87 [12800/55000 (23%)]\tLoss: 0.011779\n",
            "Train Epoch: 87 [25600/55000 (47%)]\tLoss: 0.011479\n",
            "Train Epoch: 87 [38400/55000 (70%)]\tLoss: 0.013050\n",
            "Train Epoch: 87 [51200/55000 (93%)]\tLoss: 0.011448\n",
            "\n",
            "Validation set: Avg. loss: 0.000201\n",
            "\n",
            "Train Epoch: 88 [0/55000 (0%)]\tLoss: 0.011144\n",
            "Train Epoch: 88 [12800/55000 (23%)]\tLoss: 0.013131\n",
            "Train Epoch: 88 [25600/55000 (47%)]\tLoss: 0.012907\n",
            "Train Epoch: 88 [38400/55000 (70%)]\tLoss: 0.013606\n",
            "Train Epoch: 88 [51200/55000 (93%)]\tLoss: 0.011274\n",
            "\n",
            "Validation set: Avg. loss: 0.000202\n",
            "\n",
            "Train Epoch: 89 [0/55000 (0%)]\tLoss: 0.011415\n",
            "Train Epoch: 89 [12800/55000 (23%)]\tLoss: 0.011535\n",
            "Train Epoch: 89 [25600/55000 (47%)]\tLoss: 0.009844\n",
            "Train Epoch: 89 [38400/55000 (70%)]\tLoss: 0.012344\n",
            "Train Epoch: 89 [51200/55000 (93%)]\tLoss: 0.011599\n",
            "\n",
            "Validation set: Avg. loss: 0.000201\n",
            "\n",
            "Train Epoch: 90 [0/55000 (0%)]\tLoss: 0.011984\n",
            "Train Epoch: 90 [12800/55000 (23%)]\tLoss: 0.013025\n",
            "Train Epoch: 90 [25600/55000 (47%)]\tLoss: 0.012027\n",
            "Train Epoch: 90 [38400/55000 (70%)]\tLoss: 0.012060\n",
            "Train Epoch: 90 [51200/55000 (93%)]\tLoss: 0.010869\n",
            "\n",
            "Validation set: Avg. loss: 0.000203\n",
            "\n",
            "Train Epoch: 91 [0/55000 (0%)]\tLoss: 0.012425\n",
            "Train Epoch: 91 [12800/55000 (23%)]\tLoss: 0.011472\n",
            "Train Epoch: 91 [25600/55000 (47%)]\tLoss: 0.010491\n",
            "Train Epoch: 91 [38400/55000 (70%)]\tLoss: 0.012028\n",
            "Train Epoch: 91 [51200/55000 (93%)]\tLoss: 0.012153\n",
            "\n",
            "Validation set: Avg. loss: 0.000200\n",
            "\n",
            "Train Epoch: 92 [0/55000 (0%)]\tLoss: 0.011508\n",
            "Train Epoch: 92 [12800/55000 (23%)]\tLoss: 0.012224\n",
            "Train Epoch: 92 [25600/55000 (47%)]\tLoss: 0.011258\n",
            "Train Epoch: 92 [38400/55000 (70%)]\tLoss: 0.012123\n",
            "Train Epoch: 92 [51200/55000 (93%)]\tLoss: 0.011502\n",
            "\n",
            "Validation set: Avg. loss: 0.000200\n",
            "\n",
            "Train Epoch: 93 [0/55000 (0%)]\tLoss: 0.013275\n",
            "Train Epoch: 93 [12800/55000 (23%)]\tLoss: 0.011169\n",
            "Train Epoch: 93 [25600/55000 (47%)]\tLoss: 0.011986\n",
            "Train Epoch: 93 [38400/55000 (70%)]\tLoss: 0.012172\n",
            "Train Epoch: 93 [51200/55000 (93%)]\tLoss: 0.012642\n",
            "\n",
            "Validation set: Avg. loss: 0.000201\n",
            "\n",
            "Train Epoch: 94 [0/55000 (0%)]\tLoss: 0.014225\n",
            "Train Epoch: 94 [12800/55000 (23%)]\tLoss: 0.011353\n",
            "Train Epoch: 94 [25600/55000 (47%)]\tLoss: 0.011971\n",
            "Train Epoch: 94 [38400/55000 (70%)]\tLoss: 0.012503\n",
            "Train Epoch: 94 [51200/55000 (93%)]\tLoss: 0.011695\n",
            "\n",
            "Validation set: Avg. loss: 0.000200\n",
            "\n",
            "Train Epoch: 95 [0/55000 (0%)]\tLoss: 0.011932\n",
            "Train Epoch: 95 [12800/55000 (23%)]\tLoss: 0.012403\n",
            "Train Epoch: 95 [25600/55000 (47%)]\tLoss: 0.010188\n",
            "Train Epoch: 95 [38400/55000 (70%)]\tLoss: 0.012989\n",
            "Train Epoch: 95 [51200/55000 (93%)]\tLoss: 0.011003\n",
            "\n",
            "Validation set: Avg. loss: 0.000202\n",
            "\n",
            "Train Epoch: 96 [0/55000 (0%)]\tLoss: 0.009534\n",
            "Train Epoch: 96 [12800/55000 (23%)]\tLoss: 0.010713\n",
            "Train Epoch: 96 [25600/55000 (47%)]\tLoss: 0.011470\n",
            "Train Epoch: 96 [38400/55000 (70%)]\tLoss: 0.011803\n",
            "Train Epoch: 96 [51200/55000 (93%)]\tLoss: 0.011207\n",
            "\n",
            "Validation set: Avg. loss: 0.000201\n",
            "\n",
            "Train Epoch: 97 [0/55000 (0%)]\tLoss: 0.009613\n",
            "Train Epoch: 97 [12800/55000 (23%)]\tLoss: 0.013065\n",
            "Train Epoch: 97 [25600/55000 (47%)]\tLoss: 0.013025\n",
            "Train Epoch: 97 [38400/55000 (70%)]\tLoss: 0.010917\n",
            "Train Epoch: 97 [51200/55000 (93%)]\tLoss: 0.012307\n",
            "\n",
            "Validation set: Avg. loss: 0.000200\n",
            "\n",
            "Train Epoch: 98 [0/55000 (0%)]\tLoss: 0.012221\n",
            "Train Epoch: 98 [12800/55000 (23%)]\tLoss: 0.012781\n",
            "Train Epoch: 98 [25600/55000 (47%)]\tLoss: 0.013421\n",
            "Train Epoch: 98 [38400/55000 (70%)]\tLoss: 0.011502\n",
            "Train Epoch: 98 [51200/55000 (93%)]\tLoss: 0.013584\n",
            "\n",
            "Validation set: Avg. loss: 0.000202\n",
            "\n",
            "Train Epoch: 99 [0/55000 (0%)]\tLoss: 0.013560\n",
            "Train Epoch: 99 [12800/55000 (23%)]\tLoss: 0.010828\n",
            "Train Epoch: 99 [25600/55000 (47%)]\tLoss: 0.011758\n",
            "Train Epoch: 99 [38400/55000 (70%)]\tLoss: 0.012160\n",
            "Train Epoch: 99 [51200/55000 (93%)]\tLoss: 0.012477\n",
            "\n",
            "Validation set: Avg. loss: 0.000201\n",
            "\n",
            "Train Epoch: 100 [0/55000 (0%)]\tLoss: 0.010685\n",
            "Train Epoch: 100 [12800/55000 (23%)]\tLoss: 0.012527\n",
            "Train Epoch: 100 [25600/55000 (47%)]\tLoss: 0.009726\n",
            "Train Epoch: 100 [38400/55000 (70%)]\tLoss: 0.011507\n",
            "Train Epoch: 100 [51200/55000 (93%)]\tLoss: 0.012669\n",
            "\n",
            "Validation set: Avg. loss: 0.000199\n",
            "\n",
            "\n",
            "Test set: Avg. loss: 0.000013\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ixlQG07ZiAd",
        "outputId": "78283df3-9576-4676-b7f6-5affb415e319",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "examples = enumerate(test_loader)\n",
        "batch_idx, (example_data, example_targets) = next(examples)\n",
        "\n",
        "recons_data = network(example_data.to(device)).cpu().detach().numpy()\n",
        "\n",
        "fig = plt.figure()\n",
        "for i in range(6):\n",
        "  plt.subplot(2,3,i+1)\n",
        "  plt.tight_layout()\n",
        "  plt.imshow(recons_data[i][0], cmap='gray', interpolation='none')\n",
        "  plt.title(\"Reconstructed: {}\".format(example_targets[i]))\n",
        "  plt.xticks([])\n",
        "  plt.yticks([])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 6 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9KMv6Yae_2qS",
        "outputId": "fa1f8e3e-ad37-449e-ee1e-10046fb70a09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "# Let's use the codes given by autoencode in a knn classifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "cnt=0\n",
        "for batch_idx, (data, target) in enumerate(train_loader):\n",
        "  data = data.to(device)\n",
        "  if cnt==0:\n",
        "    X = network.code(data).cpu().detach().numpy()\n",
        "    y = target.numpy()\n",
        "    cnt=1\n",
        "  else:\n",
        "    X = np.concatenate((X,network.code(data).cpu().detach().numpy()),axis=0)\n",
        "    y = np.concatenate((y,target.numpy()))\n",
        "\n",
        "\n",
        "neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "neigh.fit(X, y)\n",
        "\n",
        "correct = 0\n",
        "for batch_idx, (test_data, test_target) in enumerate(test_loader):\n",
        "  test_data = test_data.to(device)\n",
        "  x_test = network.code(test_data).cpu().detach().numpy()\n",
        "  y_pred = neigh.predict(x_test)\n",
        "  correct += np.sum(test_target.numpy()==y_pred)\n",
        "print(\"Accuracy on test data:\",correct/100,\"percent.\")"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test data: 95.61 percent.\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}