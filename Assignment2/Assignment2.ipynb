{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Assignment2.ipynb","provenance":[],"authorship_tag":"ABX9TyPE87slgAFHTShm1e3Z9O30"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SVfECaFopu8w"},"source":["Submission"]},{"cell_type":"code","metadata":{"id":"oRGoxE_CpqFb"},"source":["import torch\n","from torchvision import transforms, datasets\n","import numpy as np\n","from torch.utils.data.sampler import *\n","import torch.nn as nn\n","\n","from pprint import pformat\n","\n","torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","\n","class LogisticRegression(nn.Module):\n","    def __init__(self, params):\n","        super(LogisticRegression, self).__init__()\n","        # your code here \n","        #*** extract input dimension and output dimension from params ***#\n","        dim = params['dimension']\n","        output = params['output']\n","        self.rf = nn.Linear(dim,output) # initialize model\n","\n","\n","    def forward(self, x):\n","        out = None\n","        # your code here\n","        x = x.view(x.size(0),-1)\n","        out = self.rf(x)\n","        return out\n","\n","\n","def get_dataset(dataset_name):\n","\n","    # Ref: copied from MNIST_Multiple_Linear_Regression.ipynb on eclass to load data set\n","    #*** get both trainning and testing data from torch vision ***#\n","    if dataset_name == \"MNIST\":\n","      \n","      # Ref: copied from MNIST_Multiple_Linear_Regression.ipynb on eclass to load data set\n","      training = datasets.MNIST('/MNIST_dataset/', train=True, download=True,\n","                             transform=transforms.Compose([\n","                               transforms.ToTensor(),\n","                               transforms.Normalize((0.1307,), (0.3081,))]))\n","      # Ref: copied from MNIST_Multiple_Linear_Regression.ipynb on eclass to load data set\n","      test_set = datasets.MNIST('/MNIST_dataset/', train=False, download=True,\n","                             transform=transforms.Compose([\n","                               transforms.ToTensor(),\n","                               transforms.Normalize((0.1307,), (0.3081,))]))\n","      \n","      # spliting training_set and valid_set with specific indices\n","      indices = np.arange(0,60000)\n","      training_set = torch.utils.data.Subset(training, indices[:48000])\n","      validation_set = torch.utils.data.Subset(training, indices[48000:])\n","\n","\n","    elif dataset_name == \"CIFAR10\":\n","\n","      # Ref: copied from CIFAR10_Multiple_Linear_Regression.ipynb on eclass to load data set\n","      transform = transforms.Compose(\n","        [transforms.ToTensor(),\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","      CIFAR10_training = datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","      test_set = datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","\n","      indices = np.arange(0,50000)\n","      training_set = torch.utils.data.Subset(CIFAR10_training, indices[:38000])\n","      validation_set = torch.utils.data.Subset(CIFAR10_training, indices[38000:])\n","    else:\n","      raise Exception(dataset_name)\n","\n","    # set up batch_size for train and test\n","    batch_size_train = 128\n","    batch_size_test = 1000\n","\n","    train_dataloader = torch.utils.data.DataLoader(training_set,batch_size=batch_size_train, shuffle=True)\n","    valid_dataloader = torch.utils.data.DataLoader(validation_set,batch_size=batch_size_train, shuffle=True)\n","    test_dataloader = torch.utils.data.DataLoader(test_set,batch_size=batch_size_test, shuffle=True)\n","\n","    \n","\n","    mnist_params = {  # set parameters mnist\n","        'dimension':784,  # dimension of each input image 784 x 1\n","        'train_size':48000,     # total size of trainning set\n","        'output':10,      # output size => 10 classes\n","        'type':'mnist',     # type indicates MNIST or CIFAR10\n","        'epochs':10,      # epochs for this data set\n","        'learning_rate':3e-3,     # learning rate for the model\n","        'opt':'SGD',  # opt: which optimizer is used in this dataset\n","        'wd':1e-3,  # wd: weight value as Regularization L2\n","        'mo':0.9  # mo: Momentum value if using SGD optimizer\n","    }\n","\n","    # set parameters cifar10\n","    cifar10_params = {\n","        # your code here\n","        'dimension':3072, # dimension of each input image 784 x 1\n","        'train_size':38000,  # total size of trainning set\n","        'output':10,  # output size => 10 classes\n","        'type':'cifar', # type indicates MNIST or CIFAR10\n","        'epochs':10, # epochs for this data set\n","        'learning_rate':1e-3, # learning rate for the model\n","        'opt':'SGD',  # opt: which optimizer is used in this dataset\n","        'wd':1e-3,  # wd: weight value as Regularization L2\n","        'mo':0  # mo: Momentum value if using SGD optimizer\n","    }\n","\n","    # ******** There is nothing else under this if statement as I have already handle  ****#\n","    # ******** those two set of images at above if statement. So here is only matching ****#\n","    # ******** the parameters as default                             ****#\n","    if dataset_name == \"MNIST\":\n","        params = mnist_params\n","\n","        # your code here  ***cross out***\n","    elif dataset_name == \"CIFAR10\":\n","        params = cifar10_params\n","        # your code here  ***cross out***\n","    else:\n","        raise AssertionError(f'Invalid dataset: {dataset_name}')\n","\n","    dataloaders = {\n","        'train': train_dataloader,\n","        'valid': valid_dataloader,\n","        'test': test_dataloader,\n","    }\n","    return dataloaders, params\n","###########################################################################################################\n","\n","def test(model, test_dataloader, device, params):\n","    test_predictions = []\n","    true_labels = []\n","\n","    # your code here\n","    model.eval()  # set model to evaluation\n","\n","    # ****** This part below is adopted from lecture notebook, which calls our model ****#\n","    # ****** for output, appends them with true lables the to given lists       ****#\n","    with torch.no_grad():\n","      for data,target in test_dataloader:\n","        data = data.to(device)\n","        output = model(data)\n","        a = output.data.max(1,keepdim=False)[1]\n","        test_predictions.append(a)\n","        true_labels.append(target) \n","\n","    return test_predictions, true_labels\n","\n","\n","def validate(model, valid_dataloader, device, params):\n","    mean_acc = None\n","\n","    # your code here\n","    model.eval()  # set model to eval mode\n","    correct = 0 # used for mean_acc calculation\n","\n","    # ****** The following part is adopted from lecture notebook, multilinear regression.   ***#\n","    # ****** It is similar to test method whereas validation uses part of the training   ***#\n","    # ****** set to test the model. mean_acc is calculated by total all correct number   ***#\n","    # ****** over size of validation set                              ***#\n","\n","    with torch.no_grad():\n","      for data,target in valid_dataloader:\n","        data = data.to(device)\n","        target = target.to(device)\n","        output = model(data)\n","        pred = output.data.max(1,keepdim=True)[1]\n","        correct += pred.eq(target.data.view_as(pred)).sum()\n","\n","    correct = correct.cpu().numpy()\n","    mean_acc = correct/12500\n","    return mean_acc\n","\n","\n","def train(model, train_dataloader, valid_dataloader, device, params):\n","    mean_train_loss = 0\n","\n","    # your code here\n","\n","    # ***** extract essential value from params, learning rate, weight decay,momentum ***#\n","    lr_rate = params['learning_rate']\n","    wd = params['wd']\n","    mo = params['mo']\n","\n","    ## initialize optimizer, either SGD or Adam\n","    if params['opt'] == 'SGD':\n","      optimizer = torch.optim.SGD(model.parameters(),lr = lr_rate,  = 1e-3,momentum=mo)\n","    elif params['opt'] == 'Adam':\n","      optimizer = torch.optim.Adam(model.parameters(),lr = lr_rate)\n","\n","    epochs = params['epochs'] # define epochs value for certain dataset\n","\n","    # ****** The code below is similar to lecture notebook as it is adopted from them ***#\n","    # ****** and the difference is that cross entropy loss function being used here as ***#\n","    # ****** logistic regression model. Basiclly it gets outputs from model and target  ***# \n","    # ****** from dataset and compare with loss function, then use back prop to make  ***#\n","    # ****** model learn from each batch                            ***# \n","    for epoch in range(epochs):\n","      model.train()\n","      for batch_idx,(data,target) in enumerate(train_dataloader):\n","        data = data.to(device)\n","        target = target.to(device)\n","        optimizer.zero_grad()\n","        output = model(data)\n","        loss = nn.functional.cross_entropy(output,target)\n","        mean_train_loss += loss\n","        loss.backward()\n","        optimizer.step()\n","\n","      model.eval()\n","      if epoch%5==0:  # After each 5 epochs, call validate to prevent over fitting\n","        validate(model,valid_dataloader,device,params)\n","    \n","    mean_train_loss = mean_train_loss.detach().cpu().numpy() / params['train_size']\n","\n","    return mean_train_loss\n","\n","\n","def tune_hyper_parameter(dataloaders, device, params):\n","    \"\"\"update the following in your search\"\"\"\n","    best_optimizer = \"Adam\"\n","    best_hyperparams = {\n","        \"regularizer\": {\n","            # your code here\n","            'value':0\n","        },\n","        \"Adam\": {\n","            \"accuracy\": 0,\n","            \"learning_rate\": 0,\n","        },\n","        \"SGD\": {\n","            \"accuracy\": 0,\n","            \"learning_rate\": 0,\n","            \"momentum\": 0,\n","        }\n","    }\n","    \"\"\"accumulate all validation accuracies you compute during hyper parameter search \n","    for both optimizers\"\"\"\n","    validation_accuracy = []\n","\n","    # your code here\n","    lr_set = [1e-2,1e-3,] # my learning rate set\n","    momentum_set = [0,0.9]  # my momentum set\n","    weight_decay_set = [0,1e-3] # my weight_decay_set / regularizer\n","    mean_loss=[100,100] # store mean_loss value\n","\n","    # ****** Grid Search is used here to go through all possible combinations of hyperparameters ***#\n","    for lr in lr_set:\n","      for mo in momentum_set:\n","        for wd in weight_decay_set:\n","          # ******* For each combination, SGD optimizer is tested, then Adam ***#\n","          params['lr_rate'] = lr\n","          params['mo'] = mo\n","          params['wd'] = wd\n","          # *** SGD traning *** #\n","          params['opt'] = 'SGD'\n","          model = LogisticRegression(params).to(device) # set up trainning model\n","          c_loss = train(model,dataloaders['train'],dataloaders['valid'],device,params)  # run trainning\n","\n","          best_acc = best_hyperparams['SGD']['accuracy']\n","          acc = validate(model,dataloaders['valid'],device,params)  # check accuracy on validation\n","          \n","          if acc>best_acc and mean_loss[0] > c_loss:  # update best combination if accuacy improved\n","            best_hyperparams['SGD']['accuracy'] = acc\n","            best_hyperparams['SGD']['learning_rate'] = lr\n","            best_hyperparams['SGD']['momentum'] = mo\n","            mean_loss[0] = c_loss\n","            validation_accuracy.append(acc)\n","\n","          # ****** Now test Adam optimizer, same algorithm as above ***#\n","          # Adam training\n","          params['opt'] = 'Adam'\n","          model = LogisticRegression(params).to(device) \n","          c_lose = train(model,dataloaders['train'],dataloaders['valid'],device,params)\n","          best_acc = best_hyperparams['Adam']['accuracy']\n","          acc = validate(model,dataloaders['valid'],device,params)\n","          if acc>best_acc and mean_loss[1] > c_loss:\n","            best_hyperparams['Adam']['accuracy'] = acc\n","            best_hyperparams['Adam']['learning_rate'] = lr\n","            mean_loss[1] - c_loss\n","            validation_accuracy.append(acc)\n","    \n","    # check which optimizer combination gives best accuracy\n","    if best_hyperparams['Adam']['accuracy'] < best_hyperparams['SGD']['accuracy']:\n","      best_optimizer = 'SGD'\n","\n","\n","\n","\n","    print(\"\\nOptimal performance: Validation Accuracy: {:.3f}, \"\n","          \"with {:s} optimizer \"\n","          \"using hyper parameters:\\n{:s} \".format(\n","        max(validation_accuracy),\n","        best_optimizer,\n","        pformat(best_hyperparams[best_optimizer])))\n","\n","    print(\"\\nOptimal regularization hyper parameters:\\n{:s} \".format(\n","        pformat(best_hyperparams['regularizer'])))\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lDFL7BUEphlp"},"source":["Part1"]},{"cell_type":"code","metadata":{"id":"ABTYcXe5dcQ5","cellView":"code","executionInfo":{"status":"ok","timestamp":1601689528163,"user_tz":360,"elapsed":134015,"user":{"displayName":"菅泳钦","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisNkEPQ1MIyihCSP3C7W28PHOGAXAyhKjVek8T=s64","userId":"04446447163478496729"}},"outputId":"f13715e3-f680-4ac3-b627-893d987e0a28","colab":{"base_uri":"https://localhost:8080/","height":369}},"source":["#@title\n","import torch\n","import numpy as np\n","import timeit\n","from collections import OrderedDict\n","from pprint import pformat\n","\n","#from A2_submission import LogisticRegression, get_dataset, train, test\n","\n","\n","torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","\n","def compute_score(acc, run_time, min_thres, max_thres, max_run_time):\n","    if run_time > max_run_time:\n","        return 0.0\n","    print(run_time)\n","    if acc <= min_thres:\n","        base_score = 0.0\n","    elif acc >= max_thres:\n","        base_score = 100.0\n","    else:\n","        base_score = float(acc - min_thres) / (max_thres - min_thres) \\\n","                     * 100\n","    return base_score\n","\n","\n","def run(dataset_name, device):\n","    dataloaders, params = get_dataset(dataset_name)\n","\n","    start = timeit.default_timer()\n","\n","    model = LogisticRegression(params).to(device)\n","\n","    train(model, dataloaders['train'], dataloaders['valid'], device, params)\n","\n","    predicted_test_labels, gt_labels = test(model, dataloaders['test'], device, params)\n","\n","    if predicted_test_labels is None or gt_labels is None:\n","        return 0, 0, 0\n","\n","    stop = timeit.default_timer()\n","    run_time = stop - start\n","\n","    # np.savetxt(filename, np.asarray(predicted_test_labels))\n","\n","    correct = 0\n","    total = 0\n","    for label, prediction in zip(gt_labels, predicted_test_labels):\n","        total += label.size(0)\n","        correct += (prediction.cpu().numpy() == label.cpu().numpy()).sum().item()  # assuming your model runs on GPU\n","\n","    accuracy = float(correct) / total\n","\n","    print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total))\n","    return correct, accuracy, run_time\n","\n","\n","\"\"\"Main loop. Run time and total score will be shown below.\"\"\"\n","\n","\n","def run_on_dataset(dataset_name, device):\n","    max_run_time = 200\n","    if dataset_name == \"MNIST\":\n","        min_thres = 0.82\n","        max_thres = 0.92\n","\n","    elif dataset_name == \"CIFAR10\":\n","        min_thres = 0.28\n","        max_thres = 0.38\n","\n","    correct_predict, accuracy, run_time = run(dataset_name, device)\n","\n","    score = compute_score(accuracy, run_time, min_thres, max_thres, max_run_time)\n","    result = OrderedDict(correct_predict=correct_predict,\n","                         accuracy=accuracy, score=score,\n","                         run_time=run_time)\n","    return result, score\n","\n","\n","def main():\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","        print('Running on GPU: {}'.format(torch.cuda.get_device_name(0)))\n","    else:\n","        device = torch.device(\"cpu\")\n","        print('Running on CPU')\n","\n","    result_all = OrderedDict()\n","    score_weights = [0.5, 0.5]\n","    scores = []\n","    for dataset_name in [\"MNIST\", \"CIFAR10\"]:\n","        result_all[dataset_name], this_score = run_on_dataset(dataset_name, device)\n","        scores.append(this_score)\n","    total_score = [score * weight for score, weight in zip(scores, score_weights)]\n","    total_score = np.asarray(total_score).sum().item()\n","    result_all['total_score'] = total_score\n","    with open('result.txt', 'w') as f:\n","        f.writelines(pformat(result_all, indent=4))\n","    print(\"\\nResult:\\n\", pformat(result_all, indent=4))\n","\n","\n","if __name__ == '__main__':\n","    main()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Running on GPU: Tesla T4\n","Accuracy of the network on the 10000 test images: 92 %\n","64.47983597500024\n","Files already downloaded and verified\n","Files already downloaded and verified\n","Accuracy of the network on the 10000 test images: 38 %\n","66.65423969199992\n","\n","Result:\n"," OrderedDict([   (   'MNIST',\n","                    OrderedDict([   ('correct_predict', 9210),\n","                                    ('accuracy', 0.921),\n","                                    ('score', 100.0),\n","                                    ('run_time', 64.47983597500024)])),\n","                (   'CIFAR10',\n","                    OrderedDict([   ('correct_predict', 3836),\n","                                    ('accuracy', 0.3836),\n","                                    ('score', 100.0),\n","                                    ('run_time', 66.65423969199992)])),\n","                ('total_score', 100.0)])\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"tanAtvgHpmtX"},"source":["Part2"]},{"cell_type":"code","metadata":{"id":"Cso4_-dNpog0","executionInfo":{"status":"ok","timestamp":1601699284848,"user_tz":360,"elapsed":1192101,"user":{"displayName":"菅泳钦","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GisNkEPQ1MIyihCSP3C7W28PHOGAXAyhKjVek8T=s64","userId":"04446447163478496729"}},"outputId":"ea79626e-0c1b-45d5-f719-a96f4c63c55c","colab":{"base_uri":"https://localhost:8080/","height":228}},"source":["import torch\n","import timeit\n","from pprint import pformat\n","\n","#from A2_submission import get_dataset, tune_hyper_parameter\n","\n","torch.multiprocessing.set_sharing_strategy('file_system')\n","\n","\n","def main():\n","    if torch.cuda.is_available():\n","        device = torch.device(\"cuda\")\n","        print('Running on GPU: {}'.format(torch.cuda.get_device_name(0)))\n","    else:\n","        device = torch.device(\"cpu\")\n","        print('Running on CPU')\n","\n","    dataloaders, params = get_dataset('CIFAR10')\n","\n","    start = timeit.default_timer()\n","\n","    tune_hyper_parameter(dataloaders, device, params)\n","\n","    stop = timeit.default_timer()\n","    run_time = stop - start\n","\n","    print(\"\\nrun_time:\\n\", pformat(run_time))\n","\n","\n","if __name__ == '__main__':\n","    main()\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Running on GPU: Tesla T4\n","Files already downloaded and verified\n","Files already downloaded and verified\n","\n","Optimal performance: Validation Accuracy: 0.391, with SGD optimizer using hyper parameters:\n","{'accuracy': 0.39088, 'learning_rate': 0.01, 'momentum': 0.9} \n","\n","Optimal regularization hyper parameters:\n","{'value': 0} \n","\n","run_time:\n"," 1189.9560726139998\n"],"name":"stdout"}]}]}